{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/francescostreet/ShreckCiuchino2026/blob/main/conv_embedding_layer_choice_sistemato.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸŒ **Google Drive Connection**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "osSjRulblIiF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSIRxGoQ3pv3",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "from google.colab import drive\n",
        "drive.mount(\"/gdrive\")\n",
        "current_dir = \"/gdrive/My\\\\ Drive/Challenge1-Datasets\"\n",
        "%cd $current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## âš™ï¸ **Libraries Import**"
      ],
      "metadata": {
        "id": "XflRG2avlKhm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x47Uv8R9Akcl"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# Set seed for reproducibility\n",
        "SEED = 42\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "\n",
        "# Set environment variables before importing modules\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=Warning)\n",
        "\n",
        "# Import necessary modules\n",
        "import logging\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Set seeds for random number generators in NumPy and Python\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# Import PyTorch\n",
        "import torch\n",
        "torch.manual_seed(SEED)\n",
        "from torch import nn\n",
        "import torch.nn.functional as F # Import torch.nn.functional\n",
        "# from torchsummary import summary\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "!mkdir -p models\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "# Import other libraries\n",
        "import copy\n",
        "import shutil\n",
        "from itertools import product\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Configure plot display settings\n",
        "sns.set(font_scale=1.4)\n",
        "sns.set_style('white')\n",
        "plt.rc('font', size=14)\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## â³ **Data Loading**"
      ],
      "metadata": {
        "id": "3ohf8x1M78ea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "\n",
        "current_dir = '/gdrive/My Drive/Challenge1-Datasets'\n",
        "\n",
        "X_train = pd.read_csv(os.path.join(current_dir, 'pirate_pain_train.csv'))\n",
        "y_train = pd.read_csv(os.path.join(current_dir, 'pirate_pain_train_labels.csv'))\n",
        "X_test = pd.read_csv(os.path.join(current_dir, 'pirate_pain_test.csv'))"
      ],
      "metadata": {
        "id": "TL2TtI6Apzmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ”Ž **Exploration and Data Analysis**"
      ],
      "metadata": {
        "id": "8MiEhiCYl-R-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0m6Gn4fkm9S"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# Define column names for the dataset\n",
        "column_names = ['sample_index', 'time', 'pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4', 'n_legs', 'n_hands', 'n_eyes'] + [f'joint_{ind:02d}' for ind in range(31)]\n",
        "\n",
        "# Read the dataset into a DataFrame with specified column names\n",
        "df = pd.read_csv('pirate_pain_train.csv', header=0, names=column_names)\n",
        "\n",
        "# Merge df with y_train (the original labels DataFrame) to add the 'label' column\n",
        "df = pd.merge(df, y_train, on='sample_index', how='left')\n",
        "\n",
        "# Read the test dataset into a DataFrame with specified column names\n",
        "df_test = pd.read_csv('pirate_pain_test.csv', header=0, names=column_names)\n",
        "\n",
        "# Remove rows with any missing values\n",
        "df.dropna(axis=0, how='any', inplace=True)\n",
        "\n",
        "# Print the shape of the DataFrame\n",
        "print(f\"DataFrame shape: {df.shape}\")\n",
        "\n",
        "# Display the first 10 rows of the DataFrame\n",
        "display(df.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display a concise summary of the DataFrame\n",
        "df.info()"
      ],
      "metadata": {
        "id": "mm1slFaeu2ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data manipulation"
      ],
      "metadata": {
        "id": "cX_FZGtM8g8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define a dictionary for n_legs, n_hands, n_eyes\n",
        "legs_dictionary = df['n_legs'].unique()\n",
        "hands_dictionary = df['n_hands'].unique()\n",
        "eyes_dictionary = df['n_eyes'].unique()\n",
        "print(\"Unique strings in 'n_legs' column:\")\n",
        "print(legs_dictionary)\n",
        "print(\"Unique strings in 'n_hands' column:\")\n",
        "print(hands_dictionary)\n",
        "print(\"Unique strings in 'n_eyes' column:\")\n",
        "print(eyes_dictionary)"
      ],
      "metadata": {
        "id": "6OQBcKwIvyXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_cols = ['n_legs', 'n_hands', 'n_eyes']\n",
        "mapping = {\n",
        "    'two': 2,\n",
        "    'one+peg_leg': 1,\n",
        "    'one+hook_hand': 1,\n",
        "    'one+eye_patch': 1\n",
        "}\n",
        "\n",
        "for col in n_cols:\n",
        "    df[col] = df[col].map(mapping).astype(np.float32)\n",
        "    df_test[col] = df_test[col].map(mapping).astype(np.float32)\n",
        "\n",
        "for i in range(31):\n",
        "  df[f'joint_{i:02d}']= df[f'joint_{i:02d}'].astype(np.float32)\n",
        "  df_test[f'joint_{i:02d}']= df_test[f'joint_{i:02d}'].astype(np.float32)\n",
        "\n",
        "numeric_cols = df.select_dtypes(include='number').columns\n",
        "numeric_cols_test = df_test.select_dtypes(include='number').columns\n",
        "\n",
        "# Calcola la varianza per ogni colonna\n",
        "variance = df[numeric_cols].var()\n",
        "variance_test = df_test[numeric_cols].var()\n",
        "\n",
        "# Seleziona solo le colonne con varianza > 0\n",
        "cols_to_keep = variance[variance > 1e-3].index\n",
        "cols_to_keep_test = variance[variance > 1e-3].index\n",
        "\n",
        "# Crea un nuovo dataframe senza le colonne costanti\n",
        "df_temp = df[cols_to_keep]\n",
        "df_test_temp = df_test[cols_to_keep_test]\n",
        "\n",
        "df = df_temp\n",
        "df_test = df_test_temp\n",
        "\n",
        "# Display updated DataFrame information to confirm the changes\n",
        "df.info()\n",
        "df_test.info()"
      ],
      "metadata": {
        "id": "kxobz4YEvN0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge df with y_train (the original labels DataFrame) to add the 'label' column\n",
        "df = pd.merge(df, y_train, on='sample_index', how='left')\n",
        "\n",
        "# Remove rows with any missing values\n",
        "df.dropna(axis=0, how='any', inplace=True)\n",
        "df_test.dropna(axis=0, how='any', inplace=True)\n",
        "\n",
        "# Print the shape of the DataFrame\n",
        "print(f\"DataFrame shape: {df.shape}\")\n",
        "print(f\"Test DataFrame shape: {df_test.shape}\")\n",
        "\n",
        "# Display the first 10 rows of the DataFrame\n",
        "display(df.head(10))\n",
        "display(df_test.head(10))"
      ],
      "metadata": {
        "id": "Ew6c-eaS6RVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate descriptive statistics for numerical columns in the DataFrame\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "k377CLOSvzqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graphs"
      ],
      "metadata": {
        "id": "mbjZtkKu9OXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LAG_MAX = 100           # how many steps to check in the future values\n",
        "THRESHOLD = 0.25        # below this threshold values can be considered uncorrelated (design choice)\n",
        "\n",
        "# === CARICA IL DATASET ===\n",
        "df_graph = pd.read_csv(\"pirate_pain_train.csv\")\n",
        "\n",
        "# Seleziona solo le feature dinamiche (quelle temporali)\n",
        "features = [c for c in df_graph.columns if \"joint_\" in c or \"pain_survey_\" in c]\n",
        "\n",
        "# === CALCOLA AUTOCORRELAZIONE MEDIA SU TUTTE LE FEATURE ===\n",
        "autocorrs = []\n",
        "for lag in range(1, LAG_MAX + 1):\n",
        "    lag_corrs = []\n",
        "    for f in features:\n",
        "        try:\n",
        "            lag_corr = df_graph[f].autocorr(lag)\n",
        "            if not np.isnan(lag_corr):\n",
        "                lag_corrs.append(lag_corr)\n",
        "        except:\n",
        "            pass\n",
        "    if lag_corrs:\n",
        "        autocorrs.append(np.mean(lag_corrs))\n",
        "    else:\n",
        "        autocorrs.append(0)\n",
        "\n",
        "autocorrs = np.array(autocorrs)\n",
        "\n",
        "# find where autocorrelation < threshold\n",
        "below_threshold = np.where(autocorrs < THRESHOLD)[0]\n",
        "if len(below_threshold) > 0:\n",
        "    optimal_window = below_threshold[0] + 1  # +1 to compensate starting index\n",
        "else:\n",
        "    optimal_window = LAG_MAX\n",
        "\n",
        "print(f\"Suggested window size: {optimal_window} time steps (threshold={THRESHOLD})\")\n",
        "\n",
        "# === GRAFICO ===\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(range(1, LAG_MAX + 1), autocorrs, marker=\"o\", markersize=3)\n",
        "plt.axhline(THRESHOLD, color=\"r\", linestyle=\"--\", label=f\"Threshold {THRESHOLD}\")\n",
        "plt.axvline(optimal_window, color=\"g\", linestyle=\"--\", label=f\"Suggested window = {optimal_window}\")\n",
        "plt.title(\"Mean Autocorrelation Across Temporal Features\")\n",
        "plt.xlabel(\"Lag\")\n",
        "plt.ylabel(\"Mean Autocorrelation\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "T6J5ApWqsh7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "\n",
        "# Identify all joint columns\n",
        "joint_columns = [col for col in df_graph.columns if col.startswith('joint_')]\n",
        "\n",
        "# Create a box plot for each joint column\n",
        "plt.figure(figsize=(20, 8)) # Adjust figure size for better readability\n",
        "sns.boxplot(data=df[joint_columns])\n",
        "plt.title('Distribution of Joint Values Across All Samples')\n",
        "plt.xlabel('Joint Index')\n",
        "plt.ylabel('Value')\n",
        "plt.xticks(rotation=90) # Rotate x-axis labels for better readability\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ih1FrqLy13FW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "pain_survey_columns = [col for col in df.columns if col.startswith('pain_survey')]\n",
        "pain_survey_df = df[pain_survey_columns]\n",
        "\n",
        "print(f\"Extracted {len(pain_survey_columns)} pain survey columns.\")\n",
        "print(\"First 5 rows of the new pain_survey_df:\")\n",
        "pain_survey_df.head()\n",
        "\n",
        "pain_counts = {}\n",
        "for col in pain_survey_columns:\n",
        "    counts = pain_survey_df[col].value_counts().reindex([0, 1, 2], fill_value=0)\n",
        "    pain_counts[col] = counts.to_dict()\n",
        "\n",
        "print(\"Counts of 0, 1, and 2 for each pain_survey column:\")\n",
        "for col, counts in pain_counts.items():\n",
        "    print(f\"{col}: {counts}\")\n",
        "\n",
        "total_pain_counts = {0: 0, 1: 0, 2: 0}\n",
        "for col, counts in pain_counts.items():\n",
        "    for value, count in counts.items():\n",
        "        total_pain_counts[value] += count\n",
        "\n",
        "print(\"Total aggregated counts of 0, 1, and 2 across all pain_survey columns:\")\n",
        "print(total_pain_counts)\n",
        "\n",
        "num_pain_surveys = len(pain_survey_columns)\n",
        "num_plots = num_pain_surveys + 1 # +1 for the aggregated total\n",
        "\n",
        "# Determine grid size for subplots dynamically\n",
        "# For 5 plots, a 2x3 grid works well, leaving one empty spot\n",
        "if num_plots <= 3:\n",
        "    ncols = num_plots\n",
        "    nrows = 1\n",
        "elif num_plots <= 6:\n",
        "    ncols = 3\n",
        "    nrows = (num_plots + ncols - 1) // ncols\n",
        "else:\n",
        "    ncols = 4\n",
        "    nrows = (num_plots + ncols - 1) // ncols\n",
        "\n",
        "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(5 * ncols, 4 * nrows))\n",
        "axes = axes.flatten() # Flatten the array of axes for easy iteration\n",
        "\n",
        "# Plot individual pain_survey column counts\n",
        "for i, col in enumerate(pain_survey_columns):\n",
        "    counts_series = pd.Series(pain_counts[col])\n",
        "    counts_series.plot(kind='bar', ax=axes[i], color=['skyblue', 'lightcoral', 'lightgreen'])\n",
        "    axes[i].set_title(f'Counts for {col.replace(\"_\", \" \").title()}')\n",
        "    axes[i].set_xlabel('Value (0, 1, 2)')\n",
        "    axes[i].set_ylabel('Count')\n",
        "    axes[i].ticklabel_format(style='plain', axis='y') # Disable scientific notation\n",
        "    axes[i].grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    for p in axes[i].patches:\n",
        "        axes[i].annotate(f'{int(p.get_height()):,}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                    ha='center', va='center', xytext=(0, 5), textcoords='offset points', fontsize=9)\n",
        "\n",
        "# Plot aggregated total counts\n",
        "total_counts_series = pd.Series(total_pain_counts)\n",
        "total_counts_series.plot(kind='bar', ax=axes[num_pain_surveys], color=['skyblue', 'lightcoral', 'lightgreen'])\n",
        "axes[num_pain_surveys].set_title('Aggregated Total Counts')\n",
        "axes[num_pain_surveys].set_xlabel('Value (0, 1, 2)')\n",
        "axes[num_pain_surveys].set_ylabel('Count')\n",
        "axes[num_pain_surveys].ticklabel_format(style='plain', axis='y') # Disable scientific notation\n",
        "axes[num_pain_surveys].grid(axis='y', linestyle='--', alpha=0.7)\n",
        "for p in axes[num_pain_surveys].patches:\n",
        "    axes[num_pain_surveys].annotate(f'{int(p.get_height()):,}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                ha='center', va='center', xytext=(0, 5), textcoords='offset points', fontsize=9)\n",
        "\n",
        "# Hide any unused subplots\n",
        "for j in range(num_plots, len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('Pain Survey Value Counts', y=1.02, fontsize=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2R03fhjnnLfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data analysis & pre-processing\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RRc8gf0AztS2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "new_cell_id_1"
      },
      "source": [
        "unique_pirates = df['sample_index'].unique()\n",
        "n_pirates = len(unique_pirates)\n",
        "print(f\"Total number of pirates in the dataset: {n_pirates}\")\n",
        "\n",
        "random.seed(SEED) # Ensure reproducibility of shuffling\n",
        "random.shuffle(unique_pirates)\n",
        "\n",
        "# Define the number of users for validation and test sets\n",
        "N_VAL_USERS = 135\n",
        "\n",
        "# Calculate the number of users for the training set\n",
        "n_train_users = len(unique_pirates) - N_VAL_USERS\n",
        "\n",
        "# Split the shuffled user IDs into training and validation\n",
        "train_users = unique_pirates[:n_train_users]\n",
        "val_users = unique_pirates[n_train_users:n_train_users + N_VAL_USERS]\n",
        "\n",
        "# Split the dataset into training, validation, and test sets based on user IDs\n",
        "df_train = df[df['sample_index'].isin(train_users)]\n",
        "df_val = df[df['sample_index'].isin(val_users)]\n",
        "\n",
        "# Print the shapes of the training, validation, and test sets\n",
        "print(f'Training set shape: {df_train.shape}')\n",
        "print(f'Validation set shape: {df_val.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the columns to be normalised\n",
        "# Exclude pain_survey_X from scaling, as they will be embedded as categorical features\n",
        "scale_columns = ['n_legs', 'n_hands', 'n_eyes'] + [f'joint_{ind:02d}' for ind in range(13)] + ['joint_26', 'joint_27', 'joint_28', 'joint_29',]\n",
        "\n",
        "# Calculate the minimum and maximum values from the training data only\n",
        "mins = df_train[scale_columns].min()\n",
        "maxs = df_train[scale_columns].max()\n",
        "\n",
        "# Apply normalisation to the specified columns in all datasets\n",
        "for column in scale_columns:\n",
        "    # Normalise the training set and ensure float32\n",
        "    df_train[column] = ((df_train[column] - mins[column]) / (maxs[column] - mins[column])).astype(np.float32)\n",
        "\n",
        "    # Normalise the validation set and ensure float32\n",
        "    df_val[column] = ((df_val[column] - mins[column]) / (maxs[column] - mins[column])).astype(np.float32)\n",
        "\n",
        "    # Normalise the test set and ensure float32\n",
        "    df_test[column] = ((df_test[column] - mins[column]) / (maxs[column] - mins[column])).astype(np.float32)"
      ],
      "metadata": {
        "id": "t8cESppbCsAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Estrarre feature statiche per sample_index (una riga per soggetto) ===\n",
        "static_cols = ['n_legs', 'n_hands', 'n_eyes']\n",
        "STATIC_COLS= static_cols\n",
        "\n",
        "# Assicurati che df_train, df_val, df_test esistano giÃ  (li hai creato poco sopra)\n",
        "static_train_df = df_train.groupby('sample_index')[static_cols].first().astype(np.float32)\n",
        "static_val_df   = df_val.groupby('sample_index')[static_cols].first().astype(np.float32)\n",
        "static_test_df  = df_test.groupby('sample_index')[static_cols].first().astype(np.float32)"
      ],
      "metadata": {
        "id": "PGG0xnJMix9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first five rows of the training DataFrame\n",
        "print(df_train.shape)\n",
        "df_train.head()"
      ],
      "metadata": {
        "id": "vf0w2FmzDxA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first five rows of the test DataFrame\n",
        "print(df_test.shape)\n",
        "df_test.head()"
      ],
      "metadata": {
        "id": "Xd7E7H_9iwQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ec5AFCLqFvd0"
      },
      "outputs": [],
      "source": [
        "# Define a mapping of activity names to integer labels\n",
        "label_mapping = {\n",
        "    'no_pain': 0,\n",
        "    'low_pain': 1,\n",
        "    'high_pain': 2,\n",
        "}\n",
        "\n",
        "# Map activity names to integers in the training set\n",
        "df_train['label'] = df_train['label'].map(label_mapping)\n",
        "\n",
        "# Map activity names to integers in the validation set\n",
        "df_val['label'] = df_val['label'].map(label_mapping)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model definition"
      ],
      "metadata": {
        "id": "bqVljDtJz3sQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "WINDOW_SIZE= 60 # The choice of these values has been convalidated looking at the mean autocorrelation of the joints\n",
        "STRIDE= 20"
      ],
      "metadata": {
        "id": "MjJXSd_swuy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xy2VJmHIu6iJ"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# Define a function to build sequences from the dataset\n",
        "def build_sequences(df, window=WINDOW_SIZE, stride=STRIDE, return_labels=True, feature_columns_to_extract=None, pain_survey_columns_to_extract=None):\n",
        "    # Sanity check to ensure the window is divisible by the stride\n",
        "    assert window % stride == 0\n",
        "\n",
        "    main_features_dataset = []\n",
        "    pain_survey_features_dataset = [] # New list for pain survey features\n",
        "    labels = []\n",
        "    sequence_ids = [] # New list to store sample_index for each sequence\n",
        "\n",
        "    # Always use 'sample_index' as the ID column as per user request\n",
        "    id_column = 'sample_index'\n",
        "\n",
        "    # Always use 'label' as the label column as per user request\n",
        "    label_column = 'label'\n",
        "\n",
        "    # Use provided feature_columns_to_extract or default to all 'joint_' columns if not specified\n",
        "    if feature_columns_to_extract is None:\n",
        "        # Default to all joint and static columns if not explicitly provided\n",
        "        feature_columns_to_extract = [col for col in df.columns if col.startswith('joint_')] + STATIC_COLS\n",
        "\n",
        "    if not feature_columns_to_extract:\n",
        "        raise ValueError(\"No main feature columns provided or found for sequence building.\")\n",
        "\n",
        "    # Ensure pain_survey_columns_to_extract is a list if provided\n",
        "    if pain_survey_columns_to_extract is None:\n",
        "        pain_survey_columns_to_extract = []\n",
        "    elif isinstance(pain_survey_columns_to_extract, str):\n",
        "        pain_survey_columns_to_extract = [pain_survey_columns_to_extract]\n",
        "\n",
        "\n",
        "    for current_id in df[id_column].unique():\n",
        "        # Filter dataframe for the current ID and sort by time\n",
        "        user_data = df[df[id_column] == current_id].sort_values(by='time')\n",
        "\n",
        "        # Extract main sensor data for the current ID\n",
        "        temp_main_data = user_data[feature_columns_to_extract].values\n",
        "\n",
        "        # Extract pain survey data for the current ID\n",
        "        temp_pain_survey_data = user_data[pain_survey_columns_to_extract].values if pain_survey_columns_to_extract else np.empty((len(user_data), 0))\n",
        "\n",
        "        # Handle cases where a user might have no data points (empty temp_data)\n",
        "        if len(temp_main_data) == 0:\n",
        "            continue\n",
        "\n",
        "        current_label = None\n",
        "        if return_labels:\n",
        "            if label_column in df.columns:\n",
        "                current_label = user_data[label_column].values[0] # Label is per-pirate, so take first\n",
        "            else:\n",
        "                raise KeyError(f\"Label column '{label_column}' not found in DataFrame for ID {current_id} when labels are expected (return_labels=True).\")\n",
        "\n",
        "        # Calculate padding length to ensure full windows\n",
        "        padding_len = window - (len(temp_main_data) % window)\n",
        "        if padding_len == window: # If len(temp_main_data) is a multiple of window, no padding needed\n",
        "            padding_len = 0\n",
        "\n",
        "        # Create zero padding and concatenate with the data\n",
        "        if padding_len > 0:\n",
        "            padding_main = np.zeros((padding_len, temp_main_data.shape[1]), dtype='float32')\n",
        "            temp_main_data = np.concatenate((temp_main_data, padding_main))\n",
        "            if temp_pain_survey_data.shape[1] > 0:\n",
        "                padding_ps = np.zeros((padding_len, temp_pain_survey_data.shape[1]), dtype=temp_pain_survey_data.dtype)\n",
        "                temp_pain_survey_data = np.concatenate((temp_pain_survey_data, padding_ps))\n",
        "\n",
        "        # Build feature windows and associate them with labels\n",
        "        idx = 0\n",
        "        while idx + window <= len(temp_main_data):\n",
        "            main_features_dataset.append(temp_main_data[idx:idx + window])\n",
        "\n",
        "            # Extract pain survey features for the *first timestep* of the current window\n",
        "            if temp_pain_survey_data.shape[1] > 0:\n",
        "                pain_survey_features_dataset.append(temp_pain_survey_data[idx]) # Take the pain survey at the start of the window\n",
        "            else:\n",
        "                pain_survey_features_dataset.append(np.empty(0, dtype=np.int64)) # Empty array if no PS features\n",
        "\n",
        "            if return_labels:\n",
        "                labels.append(current_label) # Append the actual label (per pirate)\n",
        "            else:\n",
        "                labels.append(-1) # Placeholder\n",
        "\n",
        "            sequence_ids.append(current_id) # Append the current_id for each sequence\n",
        "            idx += stride\n",
        "\n",
        "    main_features_np = np.array(main_features_dataset)\n",
        "    # Ensure pain_survey_features_np has correct shape even if no PS features\n",
        "    if len(pain_survey_features_dataset) > 0 and (len(pain_survey_features_dataset[0]) > 0):\n",
        "        pain_survey_features_np = np.array(pain_survey_features_dataset)\n",
        "    else:\n",
        "        pain_survey_features_np = np.empty((len(main_features_dataset), 0), dtype=np.int64)\n",
        "\n",
        "    labels_np = np.array(labels, dtype=np.int64) if return_labels else np.empty(0, dtype=np.int64)\n",
        "    sequence_ids_np = np.array(sequence_ids)\n",
        "\n",
        "    return main_features_np, pain_survey_features_np, labels_np, sequence_ids_np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vs7s2fru6fB"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "# New parameters for pain survey embeddings\n",
        "PAIN_SURVEY_FEATURES = ['pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4']\n",
        "\n",
        "# Get the list of all feature columns (dynamic + static)\n",
        "all_feature_columns_global = [col for col in df_train.columns if col.startswith('joint_')] + STATIC_COLS\n",
        "\n",
        "# Pain survey columns\n",
        "pain_survey_cols_global = PAIN_SURVEY_FEATURES\n",
        "\n",
        "# Generate sequences and labels for the training set\n",
        "X_train, X_train_ps_sequences, y_train, seq_ids_train = build_sequences(df_train, WINDOW_SIZE, STRIDE,\n",
        "                                                                         feature_columns_to_extract=all_feature_columns_global,\n",
        "                                                                         pain_survey_columns_to_extract=pain_survey_cols_global)\n",
        "\n",
        "# Generate sequences and labels for the validation set\n",
        "X_val, X_val_ps_sequences, y_val, seq_ids_val     = build_sequences(df_val, WINDOW_SIZE, STRIDE,\n",
        "                                                                     feature_columns_to_extract=all_feature_columns_global,\n",
        "                                                                     pain_survey_columns_to_extract=pain_survey_cols_global)\n",
        "\n",
        "# Generate sequences for the test set (without labels)\n",
        "X_test, X_test_ps_sequences, _, seq_ids_test       = build_sequences(df_test, WINDOW_SIZE, STRIDE, return_labels=False,\n",
        "                                                                     feature_columns_to_extract=all_feature_columns_global,\n",
        "                                                                     pain_survey_columns_to_extract=pain_survey_cols_global)\n",
        "\n",
        "# Print the shapes of the generated datasets and their labels\n",
        "print(f\"X_train shape: {X_train.shape}, X_train_ps_sequences shape: {X_train_ps_sequences.shape}, y_train shape: {y_train.shape}\")\n",
        "print(f\"X_val shape: {X_val.shape}, X_val_ps_sequences shape: {X_val_ps_sequences.shape}, y_val shape: {y_val.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, X_test_ps_sequences shape: {X_test_ps_sequences.shape}\")\n",
        "print(f\"seq_ids_test shape: {seq_ids_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pain survey features are extracted per window by the build_sequences function.\n",
        "\n",
        "# These print statements refer to global variables that will be set by the previous step.\n",
        "print(\"X_train_ps_sequences shape:\", X_train_ps_sequences.shape)\n",
        "print(\"X_val_ps_sequences shape:\", X_val_ps_sequences.shape)\n",
        "print(\"X_test_ps_sequences shape:\", X_test_ps_sequences.shape)\n"
      ],
      "metadata": {
        "id": "cLBRBwxtxqH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkUqhy__KMnr"
      },
      "outputs": [],
      "source": [
        "# Define the input shape based on the training data\n",
        "input_shape = X_train.shape[1:]\n",
        "\n",
        "# Define the number of classes based on the categorical labels\n",
        "num_classes = len(np.unique(y_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4g715TtSytf"
      },
      "outputs": [],
      "source": [
        "train_ds = TensorDataset(\n",
        "    torch.from_numpy(X_train).float(),\n",
        "    torch.from_numpy(X_train_ps_sequences).long(),\n",
        "    torch.from_numpy(y_train).long()\n",
        ")\n",
        "val_ds = TensorDataset(\n",
        "    torch.from_numpy(X_val).float(),\n",
        "    torch.from_numpy(X_val_ps_sequences).long(),\n",
        "    torch.from_numpy(y_val).long()\n",
        ")\n",
        "# test dataset (no label)\n",
        "test_ds = TensorDataset(\n",
        "    torch.from_numpy(X_test).float(),\n",
        "    torch.from_numpy(X_test_ps_sequences).long()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjhjKSW3WysO"
      },
      "outputs": [],
      "source": [
        "# Define the batch size, which is the number of samples in each batch\n",
        "BATCH_SIZE = 32 #in this course don't exceed this, isn't useful"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIkKaZJkW0z_"
      },
      "outputs": [],
      "source": [
        "def make_loader(ds, batch_size, shuffle=True, drop_last=False, sampler=None):\n",
        "    # Determine optimal number of worker processes for data loading\n",
        "    cpu_cores = os.cpu_count() or 2\n",
        "    num_workers = max(2, min(4, cpu_cores))\n",
        "\n",
        "    # Create DataLoader with performance optimizations\n",
        "    return DataLoader(\n",
        "        ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle if sampler is None else False, # Disable shuffle if a sampler is provided\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True,  # Faster GPU transfer\n",
        "        pin_memory_device=\"cuda\" if torch.cuda.is_available() else \"\",\n",
        "        prefetch_factor=4,  # Load 4 batches ahead\n",
        "        sampler=sampler # Pass the sampler if provided\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4y83Z8VSCYxD"
      },
      "outputs": [],
      "source": [
        "# @title Weighted Random Sampler choice\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "\n",
        "# Calculate class weights for WeightedRandomSampler based on y_train\n",
        "# y_train is from the build_sequences output (numpy array of labels for each window)\n",
        "class_sample_count = np.array([(y_train == t).sum() for t in np.unique(y_train)])\n",
        "\n",
        "# Calculate class weights as total number of samples divided by class sample count\n",
        "# This scales up the weights compared to 1.0 / count, but maintains relative proportions.\n",
        "total_samples_in_y_train = len(y_train)\n",
        "class_weights_sampler = total_samples_in_y_train / class_sample_count\n",
        "\n",
        "# Create a weight for each sample in train_ds based on its class weight\n",
        "# y_train_tensor is the labels part of train_ds, but y_train (numpy) is easier to work with here\n",
        "samples_weight = np.array([class_weights_sampler[t] for t in y_train])\n",
        "samples_weight = torch.from_numpy(samples_weight).float()\n",
        "\n",
        "# Create the WeightedRandomSampler\n",
        "train_sampler = WeightedRandomSampler(\n",
        "    weights=np.sqrt(samples_weight),\n",
        "    num_samples=len(samples_weight),\n",
        "    replacement=True # Typically replacement=True for WeightedRandomSampler\n",
        ")\n",
        "\n",
        "# Create data loaders with different settings for each phase\n",
        "# Use the sampler for the training loader to handle class imbalance\n",
        "train_loader = make_loader(train_ds, batch_size=BATCH_SIZE, sampler=train_sampler, shuffle= True, drop_last=False)\n",
        "# For validation, we typically want to evaluate on the true distribution, so no sampler is used.\n",
        "val_loader   = make_loader(val_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZR5CG9qW2AI"
      },
      "outputs": [],
      "source": [
        "# @title Loader without class weights\n",
        "# Create data loaders with different settings for each phase\n",
        "train_loader = make_loader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
        "val_loader   = make_loader(val_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPpKL3_NW2dq"
      },
      "outputs": [],
      "source": [
        "for xb, xb_ps, yb in train_loader:\n",
        "    print(\"Features batch shape:\", xb.shape)\n",
        "    print(\"Pain survey features batch shape:\", xb_ps.shape)\n",
        "    print(\"Labels batch shape:\", yb.shape)\n",
        "    break # Stop after getting one batch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ› ï¸ Model Building"
      ],
      "metadata": {
        "id": "s-ZcZjWB05cE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VraH-QTDxHWD",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "def recurrent_summary(model, input_size, static_input_size=0):\n",
        "    \"\"\"\n",
        "    Custom summary function that emulates torchinfo's output while correctly\n",
        "    counting parameters for RNN/GRU/LSTM layers.\n",
        "\n",
        "    This function is designed for models whose direct children are\n",
        "    nn.Linear, nn.RNN, nn.GRU, or nn.LSTM layers.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The model to analyze.\n",
        "        input_size (tuple): Shape of the input tensor (e.g., (seq_len, features)).\n",
        "        static_input_size (int): Number of static features, if any. Defaults to 0.\n",
        "    \"\"\"\n",
        "\n",
        "    # Dictionary to store output shapes captured by forward hooks\n",
        "    output_shapes = {}\n",
        "    # List to track hook handles for later removal\n",
        "    hooks = []\n",
        "\n",
        "    def get_hook(name):\n",
        "        \"\"\"Factory function to create a forward hook for a specific module.\"\"\"\n",
        "        def hook(module, input, output):\n",
        "            # Handle RNN layer outputs (returns a tuple)\n",
        "            if isinstance(output, tuple):\n",
        "                # output[0]: all hidden states with shape (batch, seq_len, hidden*directions)\n",
        "                shape1 = list(output[0].shape)\n",
        "                shape1[0] = -1  # Replace batch dimension with -1\n",
        "\n",
        "                # output[1]: final hidden state h_n (or tuple (h_n, c_n) for LSTM)\n",
        "                if isinstance(output[1], tuple):  # LSTM case: (h_n, c_n)\n",
        "                    shape2 = list(output[1][0].shape)  # Extract h_n only\n",
        "                else:  # RNN/GRU case: h_n only\n",
        "                    shape2 = list(output[1].shape)\n",
        "\n",
        "                # Replace batch dimension (middle position) with -1\n",
        "                shape2[1] = -1\n",
        "\n",
        "                output_shapes[name] = f\"[{shape1}, {shape2}]\"\n",
        "\n",
        "            # Handle standard layer outputs (e.g., Linear)\n",
        "            else:\n",
        "                shape = list(output.shape)\n",
        "                shape[0] = -1  # Replace batch dimension with -1\n",
        "                output_shapes[name] = f\"{shape}\"\n",
        "        return hook\n",
        "\n",
        "    # 1. Determine the device where model parameters reside\n",
        "    try:\n",
        "        device = next(model.parameters()).device\n",
        "    except StopIteration:\n",
        "        device = torch.device(\"cpu\")  # Fallback for models without parameters\n",
        "\n",
        "    # 2. Create dummy input tensors with batch_size=1\n",
        "    dummy_input = torch.randn(1, *input_size).to(device)\n",
        "    dummy_static_input = None\n",
        "    if static_input_size > 0:\n",
        "        dummy_static_input = torch.randn(1, static_input_size).to(device)\n",
        "\n",
        "    # 3. Register forward hooks on target layers\n",
        "    # Iterate through direct children of the model (e.g., self.rnn, self.classifier)\n",
        "    for name, module in model.named_children():\n",
        "        if isinstance(module, (nn.Linear, nn.RNN, nn.GRU, nn.LSTM)):\n",
        "            # Register the hook and store its handle for cleanup\n",
        "            hook_handle = module.register_forward_hook(get_hook(name))\n",
        "            hooks.append(hook_handle)\n",
        "\n",
        "    # 4. Execute a dummy forward pass in evaluation mode\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        try:\n",
        "            if dummy_static_input is not None:\n",
        "                model(dummy_input, dummy_static_input)\n",
        "            else:\n",
        "                model(dummy_input) # If no static input, call with only dynamic input\n",
        "        except Exception as e:\n",
        "            print(f\"Error during dummy forward pass in recurrent_summary: {e}\")\n",
        "            # Clean up hooks even if an error occurs\n",
        "            for h in hooks:\n",
        "                h.remove()\n",
        "            return\n",
        "\n",
        "    # 5. Remove all registered hooks\n",
        "    for h in hooks:\n",
        "        h.remove()\n",
        "\n",
        "    # --- 6. Print the summary table ---\n",
        "\n",
        "    print(\"-\" * 79)\n",
        "    # Column headers\n",
        "    print(f\"{'Layer (type)':<25} {'Output Shape':<28} {'Param #':<18}\")\n",
        "    print(\"=\" * 79)\n",
        "\n",
        "    total_params = 0\n",
        "    total_trainable_params = 0\n",
        "\n",
        "    # Iterate through modules again to collect and display parameter information\n",
        "    for name, module in model.named_children():\n",
        "        if name in output_shapes:\n",
        "            # Count total and trainable parameters for this module\n",
        "            module_params = sum(p.numel() for p in module.parameters())\n",
        "            trainable_params = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
        "\n",
        "            total_params += module_params\n",
        "            total_trainable_params += trainable_params\n",
        "\n",
        "            # Format strings for display\n",
        "            layer_name = f\"{name} ({type(module).__name__})\"\n",
        "            output_shape_str = str(output_shapes[name])\n",
        "            params_str = f\"{trainable_params:,}\"\n",
        "\n",
        "            print(f\"{layer_name:<25} {output_shape_str:<28} {params_str:<15}\")\n",
        "\n",
        "    print(\"=\" * 79)\n",
        "    print(f\"Total params: {total_params:,}\")\n",
        "    print(f\"Trainable params: {total_trainable_params:,}\")\n",
        "    print(f\"Non-trainable params: {total_params - total_trainable_params:,}\")\n",
        "    print(\"-\" * 79)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogXWKej_fl6p"
      },
      "outputs": [],
      "source": [
        "# @title Model structure definition\n",
        "class RecurrentClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Generic RNN classifier (RNN, LSTM, GRU).\n",
        "    Uses the last hidden state for classification.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_size,\n",
        "            hidden_size,\n",
        "            num_layers,\n",
        "            num_classes,\n",
        "            rnn_type='LSTM',\n",
        "            bidirectional=False,\n",
        "            dropout_rate=0.2,\n",
        "            pain_survey_embedding_dims=None,\n",
        "            conv1d_out_channels=0,\n",
        "            conv1d_kernel_size=0,\n",
        "            conv1d_padding=0\n",
        "            ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.rnn_type = rnn_type\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "        self.pain_survey_embedding_dims = pain_survey_embedding_dims\n",
        "\n",
        "        self.conv1d_out_channels = conv1d_out_channels\n",
        "        self.conv1d_kernel_size = conv1d_kernel_size\n",
        "        self.conv1d_padding = conv1d_padding\n",
        "\n",
        "        # Map string name to PyTorch RNN class\n",
        "        rnn_map = {\n",
        "            'RNN': nn.RNN,\n",
        "            'LSTM': nn.LSTM,\n",
        "            'GRU': nn.GRU\n",
        "        }\n",
        "\n",
        "        if rnn_type not in rnn_map:\n",
        "            raise ValueError(\"rnn_type must be 'RNN', 'LSTM', or 'GRU'\")\n",
        "\n",
        "        rnn_module = rnn_map[rnn_type]\n",
        "\n",
        "        # Dropout is only applied between layers (if num_layers > 1)\n",
        "        dropout_val = dropout_rate if num_layers > 1 else 0\n",
        "\n",
        "        # Calculate the input size for the recursive layers\n",
        "        rnn_input_size_pre_conv = input_size\n",
        "\n",
        "        # Define the Conv1d layer if conv1d_out_channels is specified\n",
        "        if self.conv1d_out_channels > 0:\n",
        "            self.conv1d_layer = nn.Conv1d(\n",
        "                in_channels=rnn_input_size_pre_conv,\n",
        "                out_channels=self.conv1d_out_channels,\n",
        "                kernel_size=self.conv1d_kernel_size,\n",
        "                padding=self.conv1d_padding\n",
        "            )\n",
        "            rnn_input_size_actual = self.conv1d_out_channels # RNN input becomes Conv1d output channels\n",
        "        else:\n",
        "            rnn_input_size_actual = rnn_input_size_pre_conv # No Conv1d, so direct input to RNN\n",
        "\n",
        "        # Pain Survey Embeddings\n",
        "        self.pain_survey_embedding_output_size = 0\n",
        "        if self.pain_survey_embedding_dims is not None:\n",
        "            self.pain_survey_embedding_layers = nn.ModuleList([\n",
        "                nn.Embedding(num_embeddings=3, embedding_dim=emb_dim) # 3 possible values (0, 1, 2)\n",
        "                for emb_dim in self.pain_survey_embedding_dims\n",
        "            ])\n",
        "            self.pain_survey_embedding_output_size = sum(self.pain_survey_embedding_dims)\n",
        "\n",
        "        # Create the recurrent layer\n",
        "        self.rnn = rnn_module(\n",
        "            input_size=rnn_input_size_actual,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=bidirectional,\n",
        "            dropout=dropout_val\n",
        "        )\n",
        "\n",
        "        # Calculate input size for the hidden_to_classify (after RNN)\n",
        "        if self.bidirectional:\n",
        "            classifier_input_size_from_rnn = hidden_size * 2 # Concat fwd + bwd\n",
        "        else:\n",
        "            classifier_input_size_from_rnn = hidden_size\n",
        "\n",
        "        # Calculate input size for the final classifier, incorporating pain survey embeddings\n",
        "        final_classifier_input_size = classifier_input_size_from_rnn\n",
        "        if self.pain_survey_embedding_output_size > 0:\n",
        "            final_classifier_input_size += self.pain_survey_embedding_output_size\n",
        "\n",
        "        # Final classification layer as a small FFN with Leaky ReLU\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(final_classifier_input_size, final_classifier_input_size), # Hidden layer\n",
        "            nn.LeakyReLU(), # Activation for hidden layer\n",
        "            nn.Linear(final_classifier_input_size, num_classes) # Output layer\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x, x_pain_survey):\n",
        "        \"\"\"\n",
        "        x shape: (batch_size, seq_length, input_size) (dynamic + static features)\n",
        "        x_pain_survey shape: (batch_size, num_pain_survey_features) (pain survey features as integer categories)\n",
        "        \"\"\"\n",
        "        x_pre_conv_input = x\n",
        "\n",
        "        #  Apply Conv1d layer if configured\n",
        "        if self.conv1d_out_channels > 0:\n",
        "            # Conv1d expects (batch_size, channels, seq_len)\n",
        "            x_conv_input = x_pre_conv_input.permute(0, 2, 1) # (batch_size, features, seq_len)\n",
        "            x_conv_output = self.conv1d_layer(x_conv_input)\n",
        "            x_conv_output = F.relu(x_conv_output) # Apply activation after Conv1d\n",
        "\n",
        "            # Permute back to (batch_size, seq_len, features)\n",
        "            x_rnn_input_final = x_conv_output.permute(0, 2, 1) # (batch_size, seq_len, conv1d_out_channels)\n",
        "        else:\n",
        "            x_rnn_input_final = x_pre_conv_input # No Conv1d, pass direct input to RNN\n",
        "\n",
        "        # Pass the input to the RNN\n",
        "        # rnn_out shape: (batch_size, seq_len, hidden_size * num_directions)\n",
        "        # hidden shape: (num_layers * num_directions, batch_size, hidden_size)\n",
        "        rnn_out, hidden = self.rnn(x_rnn_input_final)\n",
        "\n",
        "        # Extract the last hidden state for classification\n",
        "        # LSTM returns (h_n, c_n), we only need h_n\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            hidden = hidden[0]\n",
        "\n",
        "        if self.bidirectional:\n",
        "            # Reshape to (num_layers, 2, batch_size, hidden_size)\n",
        "            hidden = hidden.view(self.num_layers, 2, -1, self.hidden_size)\n",
        "            # Concat last fwd (hidden[-1, 0, :, :]) and bwd (hidden[-1, 1, :, :])\n",
        "            hidden_to_classify = torch.cat([hidden[-1, 0, :, :], hidden[-1, 1, :, :]], dim=1) # (batch_size, hidden_size * 2)\n",
        "        else:\n",
        "            # Take the last layer's hidden state\n",
        "            hidden_to_classify = hidden[-1] # (batch_size, hidden_size)\n",
        "\n",
        "        # Concatenate pain survey embeddings, if present\n",
        "        if self.pain_survey_embedding_output_size > 0:\n",
        "            pain_survey_embeddings = []\n",
        "            for i, embedding_layer in enumerate(self.pain_survey_embedding_layers):\n",
        "                pain_survey_embeddings.append(embedding_layer(x_pain_survey[:, i]))\n",
        "            concatenated_pain_embeddings = torch.cat(pain_survey_embeddings, dim=1)\n",
        "            hidden_to_classify = torch.cat((hidden_to_classify, concatenated_pain_embeddings), dim=1)\n",
        "\n",
        "        # Get logits from the sequential classifier\n",
        "        logits = self.classifier(hidden_to_classify)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Network and training hypeprparameters"
      ],
      "metadata": {
        "id": "etslsY681Fzs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxtAqB60mcAd"
      },
      "outputs": [],
      "source": [
        "LEARNING_RATE = 1e-3\n",
        "EPOCHS = 400\n",
        "PATIENCE = 50\n",
        "\n",
        "HIDDEN_LAYERS = 2\n",
        "HIDDEN_SIZE = 128\n",
        "\n",
        "DROPOUT_RATE = 0.3\n",
        "L1_LAMBDA = 1e-4\n",
        "# set L2 != 1e-2 if default value of AdamW isn't good\n",
        "L2_LAMBDA = 1e-4\n",
        "\n",
        "LABEL_SMOOTHING= 0.05\n",
        "GRADIENT_CLIPPING_MAX_NORM = None # New hyperparameter for gradient clipping\n",
        "\n",
        "# Parameters for pain survey embeddings: if not desired, set to None the correspendoning parameter\n",
        "# in the model creation\n",
        "PAIN_SURVEY_FEATURES = ['pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4']\n",
        "# Define embedding dimensions for each pain survey feature\n",
        "PAIN_SURVEY_EMBEDDING_DIMS = [3, 3, 3, 3] # 3 values: 0,1,2 for each pain survey\n",
        "\n",
        "# Parameters for Conv1d layer: if not desired, set to 0\n",
        "CONV1D_OUT_CHANNELS = 24 # Number of output features from Conv1d\n",
        "CONV1D_KERNEL_SIZE = 3   # Kernel size for Conv1d\n",
        "CONV1D_PADDING = 1       # Padding for Conv1d (to maintain sequence length for kernel size 3)\n",
        "\n",
        "# Calculate class weights for the imbalanced dataset\n",
        "class_labels = np.unique(y_train) # Use the global y_train (numpy array) that has integer labels\n",
        "class_weights_np = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=class_labels,\n",
        "    y=y_train\n",
        ")\n",
        "\n",
        "# optional: reduce the weights\n",
        "class_weights_np = np.sqrt(class_weights_np)\n",
        "# to test with non-weighted cross entropy\n",
        "one_weights= np.ones_like(class_weights_np)\n",
        "class_weights_tensor = torch.tensor(class_weights_np, dtype=torch.float32).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor, label_smoothing=LABEL_SMOOTHING)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxUcNEn7oJ9-"
      },
      "outputs": [],
      "source": [
        "# Initialize best model tracking variables\n",
        "best_model = None\n",
        "best_performance = float('-inf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3TgsfOzoMeW"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "def train_one_epoch(model, train_loader, criterion, optimizer, scaler, device, l1_lambda=L1_LAMBDA, l2_lambda=L2_LAMBDA, gradient_clipping_max_norm=None):\n",
        "    \"\"\"\n",
        "    Perform one complete training epoch through the entire training dataset.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The neural network model to train\n",
        "        train_loader (DataLoader): PyTorch DataLoader containing training data batches\n",
        "        criterion (nn.Module): Loss function (e.g., CrossEntropyLoss, MSELoss)\n",
        "        optimizer (torch.optim): Optimization algorithm (e.g., Adam, SGD)\n",
        "        scaler (GradScaler): PyTorch's gradient scaler for mixed precision training\n",
        "        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n",
        "        l1_lambda (float): Lambda for L1 regularization\n",
        "        l2_lambda (float): Lambda for L2 regularization\n",
        "        gradient_clipping_max_norm (float, optional): Max norm for gradient clipping. If None, no clipping.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (average_loss, f1 score) - Training loss and f1 score for this epoch\n",
        "    \"\"\"\n",
        "    model.train()  # Set model to training mode\n",
        "\n",
        "    running_loss = 0.0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    # Iterate through training batches\n",
        "    for batch_idx, (inputs, pain_survey_inputs, targets) in enumerate(train_loader):\n",
        "        # Move data to device (GPU/CPU)\n",
        "        inputs, pain_survey_inputs, targets = inputs.to(device), pain_survey_inputs.to(device), targets.to(device)\n",
        "\n",
        "        # Clear gradients from previous step\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        # Forward pass with mixed precision (if CUDA available)\n",
        "        with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
        "            logits = model(inputs, pain_survey_inputs)\n",
        "            loss = criterion(logits, targets)\n",
        "\n",
        "            # Add L1 and L2 regularization only to RNN and Classifier parameters; design choice\n",
        "            l1_norm_rnn = sum(p.abs().sum() for p in model.rnn.parameters())\n",
        "            l1_norm_classifier = sum(p.abs().sum() for p in model.classifier.parameters())\n",
        "            l1_norm = l1_norm_rnn + l1_norm_classifier\n",
        "\n",
        "            l2_norm_rnn = sum(p.pow(2).sum() for p in model.rnn.parameters())\n",
        "            l2_norm_classifier = sum(p.pow(2).sum() for p in model.classifier.parameters())\n",
        "            l2_norm = l2_norm_rnn + l2_norm_classifier\n",
        "\n",
        "            loss = loss + l1_lambda * l1_norm #+ l2_lambda * l2_norm not necessary with AdamW\n",
        "\n",
        "        # Backward pass with gradient scaling\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # Unscale gradients before clipping\n",
        "        if gradient_clipping_max_norm is not None:\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=gradient_clipping_max_norm)\n",
        "\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # Accumulate metrics\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        predictions = logits.argmax(dim=1)\n",
        "        all_predictions.append(predictions.cpu().numpy())\n",
        "        all_targets.append(targets.cpu().numpy())\n",
        "\n",
        "    # Calculate epoch metrics\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    epoch_f1 = f1_score(\n",
        "        np.concatenate(all_targets),\n",
        "        np.concatenate(all_predictions),\n",
        "        average='weighted'\n",
        "    )\n",
        "\n",
        "    return epoch_loss, epoch_f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iR7Oa-m4oOm2"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "def validate_one_epoch(model, val_loader, criterion, device):\n",
        "    \"\"\"\n",
        "    Perform one complete validation epoch through the entire validation dataset.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The neural network model to evaluate (must be in eval mode)\n",
        "        val_loader (DataLoader): PyTorch DataLoader containing validation data batches\n",
        "        criterion (nn.Module): Loss function used to calculate validation loss\n",
        "        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n",
        "\n",
        "    Returns:\n",
        "        tuple: (average_loss, accuracy) - Validation loss and accuracy for this epoch\n",
        "\n",
        "    Note:s\n",
        "        This function automatically sets the model to evaluation mode and disables\n",
        "        gradient computation for efficiency during validation.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    running_loss = 0.0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    # Disable gradient computation for validation\n",
        "    with torch.no_grad():\n",
        "        for inputs, pain_survey_inputs, targets in val_loader:\n",
        "            # Move data to device\n",
        "            inputs, pain_survey_inputs, targets = inputs.to(device), pain_survey_inputs.to(device), targets.to(device)\n",
        "\n",
        "            # Forward pass with mixed precision (if CUDA available)\n",
        "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
        "                logits = model(inputs, pain_survey_inputs)\n",
        "                loss = criterion(logits, targets)\n",
        "\n",
        "            # Accumulate metrics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            predictions = logits.argmax(dim=1)\n",
        "            all_predictions.append(predictions.cpu().numpy())\n",
        "            all_targets.append(targets.cpu().numpy())\n",
        "\n",
        "    # Calculate epoch metrics\n",
        "    epoch_loss = running_loss / len(val_loader.dataset)\n",
        "    epoch_accuracy = f1_score(\n",
        "        np.concatenate(all_targets),\n",
        "        np.concatenate(all_predictions),\n",
        "        average='weighted'\n",
        "    )\n",
        "\n",
        "    return epoch_loss, epoch_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ujrKlgKoQ3s",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "def log_metrics_to_tensorboard(writer, epoch, train_loss, train_f1, val_loss, val_f1, model):\n",
        "    \"\"\"\n",
        "    Log training metrics and model parameters to TensorBoard for visualization.\n",
        "\n",
        "    Args:\n",
        "        writer (SummaryWriter): TensorBoard SummaryWriter object for logging\n",
        "        epoch (int): Current epoch number (used as x-axis in TensorBoard plots)\n",
        "        train_loss (float): Training loss for this epoch\n",
        "        train_f1 (float): Training f1 score for this epoch\n",
        "        val_loss (float): Validation loss for this epoch\n",
        "        val_f1 (float): Validation f1 score for this epoch\n",
        "        model (nn.Module): The neural network model (for logging weights/gradients)\n",
        "\n",
        "    Note:\n",
        "        This function logs scalar metrics (loss/f1 score) and histograms of model\n",
        "        parameters and gradients, which helps monitor training progress and detect\n",
        "        issues like vanishing/exploding gradients.\n",
        "    \"\"\"\n",
        "    # Log scalar metrics\n",
        "    writer.add_scalar('Loss/Training', train_loss, epoch)\n",
        "    writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
        "    writer.add_scalar('F1/Training', train_f1, epoch)\n",
        "    writer.add_scalar('F1/Validation', val_f1, epoch)\n",
        "\n",
        "    # Log model parameters and gradients\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            # Check if the tensor is not empty and has variance before adding a histogram\n",
        "            if param.numel() > 0 and param.data.std() > 0:\n",
        "                writer.add_histogram(f'{name}/weights', param.data, epoch)\n",
        "            if param.grad is not None:\n",
        "                # Check if the gradient tensor is not empty, finite, and has variance before adding a histogram\n",
        "                if param.grad.numel() > 0 and torch.isfinite(param.grad).all() and param.grad.data.std() > 0:\n",
        "                    writer.add_histogram(f'{name}/gradients', param.grad.data, epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRCpArKIoS2C",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "def fit(model, train_loader, val_loader, epochs, criterion, optimizer, scaler, device,\n",
        "        l1_lambda=L1_LAMBDA, l2_lambda=L2_LAMBDA, patience=0, evaluation_metric=\"val_f1\", mode='max',\n",
        "        restore_best_weights=True, writer=None, verbose=10, experiment_name=\"\", gradient_clipping_max_norm=None):\n",
        "    \"\"\"\n",
        "    Train the neural network model on the training data and validate on the validation data.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The neural network model to train\n",
        "        train_loader (DataLoader): PyTorch DataLoader containing training data batches\n",
        "        val_loader (DataLoader): PyTorch DataLoader containing validation data batches\n",
        "        epochs (int): Number of training epochs\n",
        "        criterion (nn.Module): Loss function (e.g., CrossEntropyLoss, MSELoss)\n",
        "        optimizer (torch.optim): Optimization algorithm (e.g., Adam, SGD)\n",
        "        scaler (GradScaler): PyTorch's gradient scaler for mixed precision training\n",
        "        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n",
        "        l1_lambda (float): L1 regularization coefficient (default: 0)\n",
        "        l2_lambda (float): L2 regularization coefficient (default: 0)\n",
        "        patience (int): Number of epochs to wait for improvement before early stopping (default: 0)\n",
        "        evaluation_metric (str): Metric to monitor for early stopping (default: \"val_f1\")\n",
        "        mode (str): 'max' for maximizing the metric, 'min' for minimizing (default: 'max')\n",
        "        restore_best_weights (bool): Whether to restore model weights from best epoch (default: True)\n",
        "        writer (SummaryWriter, optional): TensorBoard SummaryWriter object for logging (default: None)\n",
        "        verbose (int, optional): Frequency of printing training progress (default: 10)\n",
        "        experiment_name (str, optional): Experiment name for saving models (default: \"\")\n",
        "        gradient_clipping_max_norm (float, optional): Max norm for gradient clipping. If None, no clipping.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (model, training_history) - Trained model and metrics history\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize metrics tracking\n",
        "    training_history = {\n",
        "        'train_loss': [], 'val_loss': [],\n",
        "        'train_f1': [], 'val_f1': []\n",
        "    }\n",
        "\n",
        "    # Configure early stopping if patience is set\n",
        "    if patience > 0:\n",
        "        patience_counter = 0\n",
        "        best_metric = float('-inf') if mode == 'max' else float('inf')\n",
        "        best_epoch = 0\n",
        "\n",
        "    print(f\"Training {epochs} epochs...\")\n",
        "\n",
        "    # Main training loop: iterate through epochs\n",
        "    for epoch in range(1, epochs + 1):\n",
        "\n",
        "        # Forward pass through training data, compute gradients, update weights\n",
        "        train_loss, train_f1 = train_one_epoch(\n",
        "            model, train_loader, criterion, optimizer, scaler, device, l1_lambda, l2_lambda, gradient_clipping_max_norm\n",
        "        )\n",
        "\n",
        "        # Evaluate model on validation data without updating weights\n",
        "        val_loss, val_f1 = validate_one_epoch(\n",
        "            model, val_loader, criterion, device\n",
        "        )\n",
        "\n",
        "        # Store metrics for plotting and analysis\n",
        "        training_history['train_loss'].append(train_loss)\n",
        "        training_history['val_loss'].append(val_loss)\n",
        "        training_history['train_f1'].append(train_f1)\n",
        "        training_history['val_f1'].append(val_f1)\n",
        "\n",
        "        # Write metrics to TensorBoard for visualization\n",
        "        if writer is not None:\n",
        "            log_metrics_to_tensorboard(\n",
        "                writer, epoch, train_loss, train_f1, val_loss, val_f1, model\n",
        "            )\n",
        "\n",
        "        # Print progress every N epochs or on first epoch\n",
        "        if verbose > 0:\n",
        "            if epoch % verbose == 0 or epoch == 1:\n",
        "                print(f\"Epoch {epoch:3d}/{epochs} | \"\n",
        "                    f\"Train: Loss={train_loss:.4f}, F1 Score={train_f1:.4f} | \"\n",
        "                    f\"Val: Loss={val_loss:.4f}, F1 Score={val_f1:.4f}\")\n",
        "\n",
        "        # Early stopping logic: monitor metric and save best model\n",
        "        if patience > 0:\n",
        "            current_metric = training_history[evaluation_metric][-1]\n",
        "            is_improvement = (current_metric > best_metric) if mode == 'max' else (current_metric < best_metric)\n",
        "\n",
        "            if is_improvement:\n",
        "                best_metric = current_metric\n",
        "                best_epoch = epoch\n",
        "                torch.save(model.state_dict(), \"models/\"+experiment_name+'_model.pt')\n",
        "                patience_counter = 0\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= patience:\n",
        "                    print(f\"Early stopping triggered after {epoch} epochs.\")\n",
        "                    break\n",
        "\n",
        "    # Restore best model weights if early stopping was used\n",
        "    if restore_best_weights and patience > 0:\n",
        "        model.load_state_dict(torch.load(\"models/\"+experiment_name+'_model.pt'))\n",
        "        print(f\"Best model restored from epoch {best_epoch} with {evaluation_metric} {best_metric:.4f}\")\n",
        "\n",
        "    # Save final model if no early stopping\n",
        "    if patience == 0:\n",
        "        torch.save(model.state_dict(), \"models/\"+experiment_name+'_model.pt')\n",
        "\n",
        "    # Close TensorBoard writer\n",
        "    if writer is not None:\n",
        "        writer.close()\n",
        "\n",
        "    return model, training_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlcIAz70dt87"
      },
      "outputs": [],
      "source": [
        "combined_input_features = X_train.shape[-1] # Use shape from X_train, since it contains both static and dynamic features\n",
        "\n",
        "# Create model and display architecture with parameter count\n",
        "rnn_model = RecurrentClassifier(\n",
        "    input_size=combined_input_features,\n",
        "    hidden_size=HIDDEN_SIZE,\n",
        "    num_layers=HIDDEN_LAYERS,\n",
        "    num_classes=num_classes,\n",
        "    dropout_rate=DROPOUT_RATE,\n",
        "    bidirectional=True,\n",
        "    rnn_type='LSTM',\n",
        "    pain_survey_embedding_dims=PAIN_SURVEY_EMBEDDING_DIMS,\n",
        "    conv1d_out_channels=CONV1D_OUT_CHANNELS,\n",
        "    conv1d_kernel_size=CONV1D_KERNEL_SIZE,\n",
        "    conv1d_padding=CONV1D_PADDING\n",
        "    ).to(device)\n",
        "\n",
        "\n",
        "# Conv1d Parameters: (in_channels * kernel_size + 1) * out_channels; +1 for the bias\n",
        "conv1d_params = (combined_input_features * CONV1D_KERNEL_SIZE + 1) * CONV1D_OUT_CHANNELS\n",
        "print(f\"{'conv1d_layer (Conv1d)':<25} {{'[-1, {CONV1D_OUT_CHANNELS}, 60]':<28}} {conv1d_params:,<18}\")\n",
        "\n",
        "# Pain Survey Embeddings Parameters\n",
        "pain_survey_embedding_output_size = sum(PAIN_SURVEY_EMBEDDING_DIMS)\n",
        "pain_survey_embed_params = sum([3 * emb_dim for emb_dim in PAIN_SURVEY_EMBEDDING_DIMS]) # (0,1,2) for each survey\n",
        "print(f\"{'pain_survey_embed (Embedding)':<25} {{'[-1, {pain_survey_embedding_output_size}]':<28}} {pain_survey_embed_params:,<18}\")\n",
        "\n",
        "# Calculate RNN parameters with Conv1d output as its input_size\n",
        "rnn_input_size_after_conv = CONV1D_OUT_CHANNELS\n",
        "hidden_size = HIDDEN_SIZE\n",
        "num_layers = HIDDEN_LAYERS\n",
        "rnn_layer1_params = 2 * 4 * (rnn_input_size_after_conv * hidden_size + hidden_size * hidden_size + hidden_size)\n",
        "rnn_other_layers_params = (num_layers - 1) * 2 * 4 * (2 * hidden_size * hidden_size + hidden_size * hidden_size + hidden_size)\n",
        "rnn_total_params = rnn_layer1_params + rnn_other_layers_params\n",
        "rnn_output_shape_str = f\"[[-1, {WINDOW_SIZE}, {hidden_size*2}], [{num_layers*2}, -1, {hidden_size}]]\"\n",
        "print(f\"{'rnn (LSTM)':<25} {rnn_output_shape_str:<28} {rnn_total_params:,<18}\")\n",
        "\n",
        "# Classifier parameters: Adjusted to include pain survey embedding size\n",
        "classifier_input_size = (HIDDEN_SIZE * 2) + pain_survey_embedding_output_size\n",
        "classifier_params = (classifier_input_size * classifier_input_size + classifier_input_size) + (classifier_input_size * num_classes + num_classes)\n",
        "print(f\"{'classifier (Sequential)':<25} {{'[-1, 3]':<28}} {classifier_params:,<18}\")\n",
        "\n",
        "total_params = conv1d_params + pain_survey_embed_params + rnn_total_params + classifier_params\n",
        "print(\"===============================================================================\")\n",
        "print(f\"Total params: {total_params:,}\")\n",
        "print(f\"Trainable params: {total_params:,}\")\n",
        "print(f\"Non-trainable params: 0\")\n",
        "print(\"-------------------------------------------------------------------------------\")\n",
        "\n",
        "# Set up TensorBoard logging and save model architecture\n",
        "experiment_name = \"conv1d_embed_ls_l1l2_sqrtcw\"\n",
        "writer = SummaryWriter(\"./\"+logs_dir+\"/\"+experiment_name)\n",
        "x_dummy = torch.randn(1, WINDOW_SIZE, combined_input_features).to(device) # Use WINDOW_SIZE for seq_len\n",
        "x_ps_dummy = torch.randint(0, 3, (1, len(PAIN_SURVEY_FEATURES))).long().to(device) # Dummy input for pain surveys\n",
        "writer.add_graph(rnn_model, (x_dummy, x_ps_dummy))\n",
        "\n",
        "# Define optimizer with L2 regularization;\n",
        "optimizer = torch.optim.AdamW(rnn_model.parameters(), lr=LEARNING_RATE, weight_decay=L2_LAMBDA)\n",
        "\n",
        "# Enable mixed precision training for GPU acceleration\n",
        "scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRAINING"
      ],
      "metadata": {
        "id": "qf4VWiRX1kWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logs_dir = \"tensorboard\"\n",
        "!pkill -f tensorboard\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir {logs_dir}/{experiment_name}"
      ],
      "metadata": {
        "id": "osnlJVm59IKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDLP0xlEoUNX"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Train model and track training history\n",
        "rnn_model, training_history = fit(\n",
        "    model=rnn_model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    epochs=EPOCHS,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    scaler=scaler,\n",
        "    device=device,\n",
        "    writer=writer,\n",
        "    verbose=10,\n",
        "    experiment_name=experiment_name,\n",
        "    patience=PATIENCE,\n",
        "    gradient_clipping_max_norm=GRADIENT_CLIPPING_MAX_NORM\n",
        "    )\n",
        "\n",
        "# Update best_performance with the actual best F1 score from training history\n",
        "# The fit function restores the model to the best state, so best_performance should reflect that.\n",
        "if training_history['val_f1'] and max(training_history['val_f1']) > best_performance:\n",
        "    best_performance = max(training_history['val_f1'])\n",
        "    print(f\"Updated best_performance to: {best_performance:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25ab7740"
      },
      "source": [
        "# Define the path to save the best model with a descriptive filename\n",
        "output_model_path = f\"models/{experiment_name}_best_model_f1_{best_performance:.4f}.pt\"\n",
        "\n",
        "# Save the state dictionary of the rnn_model (which holds the best weights)\n",
        "torch.save(rnn_model.state_dict(), output_model_path)\n",
        "\n",
        "print(f\"Best model saved successfully to: {output_model_path}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93495ed8"
      },
      "source": [
        "# @title\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Create an inverse mapping from numeric labels back to string labels\n",
        "inverse_label_mapping = {\n",
        "    0: 'no_pain',\n",
        "    1: 'low_pain',\n",
        "    2: 'high_pain'\n",
        "}\n",
        "\n",
        "# Create a TensorDataset and DataLoader for the test set\n",
        "# test_ds is now configured to pass combined X_test and X_test_ps_sequences\n",
        "test_loader = make_loader(test_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "rnn_model.eval() # This assumes rnn_model is the model to be evaluated\n",
        "\n",
        "all_test_predictions = []\n",
        "# The seq_ids_test array generated by build_sequences already maps sequence predictions to sample_index.\n",
        "# We will use this directly for grouping after predictions.\n",
        "\n",
        "# Perform inference on the test set\n",
        "with torch.no_grad():\n",
        "    for inputs_batch, pain_survey_inputs_batch in test_loader:\n",
        "        inputs_batch = inputs_batch.to(device)\n",
        "        pain_survey_inputs_batch = pain_survey_inputs_batch.to(device)\n",
        "\n",
        "        # Get model outputs\n",
        "        outputs = rnn_model(inputs_batch, pain_survey_inputs_batch)\n",
        "\n",
        "        # Get predicted class (the one with the highest probability)\n",
        "        predictions = outputs.argmax(dim=1).cpu().numpy()\n",
        "\n",
        "        all_test_predictions.extend(predictions)\n",
        "\n",
        "# Combine seq_ids_test and predictions into a DataFrame\n",
        "predictions_df = pd.DataFrame({\n",
        "    'sample_index': seq_ids_test, # Use the global seq_ids_test array\n",
        "    'predicted_label_numeric': all_test_predictions\n",
        "})\n",
        "\n",
        "# Group by sample_index and determine the final prediction (e.g., by taking the mode)\n",
        "# Using pandas Series.mode() which is typically more robust for Series.\n",
        "# Add a check for empty group to avoid IndexError.\n",
        "final_predictions_grouped = predictions_df.groupby('sample_index')['predicted_label_numeric'].apply(\n",
        "    lambda x: x.mode()[0] if not x.empty else np.nan # If a group is empty, assign NaN\n",
        ")\n",
        "final_predictions_df = final_predictions_grouped.reset_index()\n",
        "final_predictions_df.columns = ['sample_index', 'predicted_label_numeric']\n",
        "\n",
        "# Map numeric predictions back to string labels\n",
        "final_predictions_df['predicted_label'] = final_predictions_df['predicted_label_numeric'].map(inverse_label_mapping)\n",
        "\n",
        "# Define the output filename\n",
        "output_filename = f'pirate_pain_predictions_{experiment_name}.csv'\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "final_predictions_df[['sample_index', 'predicted_label']].to_csv(output_filename, index=False)\n",
        "\n",
        "print(f\"Predictions saved to {output_filename}\")\n",
        "display(final_predictions_df.head())\n",
        "\n",
        "# The file is already saved to Google Drive because the current working directory\n",
        "# is set to '/gdrive/My Drive/Challenge1-Datasets'. No need to copy it again.\n",
        "print(f\"File '{output_filename}' is already saved to Google Drive at '{current_dir}/{output_filename}'\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_d6Nm475L3MB",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Plot History\n",
        "# Create a figure with two side-by-side subplots (two columns)\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(18, 5))\n",
        "\n",
        "# Plot of training and validation loss on the first axis\n",
        "ax1.plot(training_history['train_loss'], label='Training loss', alpha=0.3, color='#ff7f0e', linestyle='--')\n",
        "ax1.plot(training_history['val_loss'], label='Validation loss', alpha=0.9, color='#ff7f0e')\n",
        "ax1.set_title('Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(alpha=0.3)\n",
        "\n",
        "# Plot of training and validation accuracy on the second axis\n",
        "ax2.plot(training_history['train_f1'], label='Training f1', alpha=0.3, color='#ff7f0e', linestyle='--')\n",
        "ax2.plot(training_history['val_f1'], label='Validation f1', alpha=0.9, color='#ff7f0e')\n",
        "ax2.set_title('F1 Score')\n",
        "ax2.legend()\n",
        "ax2.grid(alpha=0.3)\n",
        "\n",
        "# Adjust the layout and display the plot\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(right=0.85)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pvzW_GHe1Wf",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Plot Confusion Matrix\n",
        "# Collect predictions and ground truth labels\n",
        "val_preds, val_targets = [], []\n",
        "with torch.no_grad():  # Disable gradient computation for inference\n",
        "    for xb, xb_ps, yb in val_loader:\n",
        "        xb = xb.to(device)\n",
        "        xb_ps = xb_ps.to(device)\n",
        "\n",
        "        # Forward pass: get model predictions\n",
        "        logits = rnn_model(xb, xb_ps)\n",
        "        preds = logits.argmax(dim=1).cpu().numpy()\n",
        "\n",
        "        # Store batch results\n",
        "        val_preds.append(preds)\n",
        "        val_targets.append(yb.numpy())\n",
        "\n",
        "# Combine all batches into single arrays\n",
        "val_preds = np.concatenate(val_preds)\n",
        "val_targets = np.concatenate(val_targets)\n",
        "\n",
        "# Calculate overall validation metrics\n",
        "val_acc = accuracy_score(val_targets, val_preds)\n",
        "val_prec = precision_score(val_targets, val_preds, average='weighted', zero_division=0)\n",
        "val_rec = recall_score(val_targets, val_preds, average='weighted', zero_division=0)\n",
        "val_f1 = f1_score(val_targets, val_preds, average='weighted', zero_division=0)\n",
        "print(f\"Accuracy over the validation set: {val_acc:.4f}\")\n",
        "print(f\"Precision over the validation set: {val_prec:.4f}\")\n",
        "print(f\"Recall over the validation set: {val_rec:.4f}\")\n",
        "print(f\"F1 score over the validation set: {val_f1:.4f}\")\n",
        "\n",
        "# Generate confusion matrix for detailed error analysis\n",
        "cm = confusion_matrix(val_targets, val_preds)\n",
        "\n",
        "# Create numeric labels for heatmap annotation\n",
        "labels = np.array([f\"{num}\" for num in cm.flatten()]).reshape(cm.shape)\n",
        "\n",
        "# Visualise confusion matrix\n",
        "plt.figure(figsize=(8, 7))\n",
        "sns.heatmap(cm, annot=labels, fmt='',\n",
        "            cmap='Blues')\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Confusion Matrix â€” Validation Set')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CROSS VALIDATION"
      ],
      "metadata": {
        "id": "K1DgKU6Q2R6u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8VYpD2qHXYf"
      },
      "outputs": [],
      "source": [
        "# Cross-validation\n",
        "K = 5                    # Number of splits (5 and 10 are considered good values)\n",
        "N_VAL_USERS = 100          # Number of users for validation split\n",
        "\n",
        "# Training\n",
        "EPOCHS = 500             # Maximum epochs (increase to improve performance)\n",
        "PATIENCE = 50            # Early stopping patience (increase to improve performance)\n",
        "VERBOSE = 10             # Print frequency\n",
        "\n",
        "# Optimisation\n",
        "LEARNING_RATE = 1e-3     # Learning rate\n",
        "BATCH_SIZE = 32         # Batch size\n",
        "WINDOW_SIZE = 60       # Input window size\n",
        "STRIDE = 20            # Input stride\n",
        "\n",
        "# Architecture\n",
        "HIDDEN_LAYERS = 2        # Hidden layers\n",
        "HIDDEN_SIZE = 64        # Neurons per layer\n",
        "RNN_TYPE = 'LSTM'         # Type of RNN architecture\n",
        "BIDIRECTIONAL = True    # Bidirectional RNN\n",
        "\n",
        "# Regularisation\n",
        "DROPOUT_RATE = 0.3       # Dropout probability\n",
        "L1_LAMBDA = 0           # L1 penalty\n",
        "L2_LAMBDA = 1e-4            # L2 penalty\n",
        "\n",
        "LABEL_SMOOTHING= 0.05\n",
        "\n",
        "GRADIENT_CLIPPING_MAX_NORM = 0.5\n",
        "\n",
        "# weights are the same as the ones selected in the \"Network and training hyperparameters\" cell\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor, label_smoothing=LABEL_SMOOTHING)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFfH7LKpHXWH"
      },
      "outputs": [],
      "source": [
        "def k_shuffle_split_cross_validation_round_rnn(df, epochs, criterion, device,\n",
        "                            k, n_val_users, batch_size, hidden_layers, hidden_size, learning_rate, dropout_rate,\n",
        "                            window_size, stride, rnn_type, bidirectional,\n",
        "                            l1_lambda, l2_lambda, patience, evaluation_metric=\"val_f1\", mode='max',\n",
        "                            restore_best_weights=True, writer=None, verbose=10, seed=42, experiment_name=\"\",\n",
        "                            gradient_clipping_max_norm=None, pain_survey_embedding_dims=None):\n",
        "    \"\"\"\n",
        "    Perform K-fold shuffle split cross-validation with user-based splitting for time series data.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame with columns ['sample_index', 'label', 'pain_survey_X', 'n_X', 'joint_XX']\n",
        "        epochs: Number of training epochs\n",
        "        criterion: Loss function\n",
        "        device: torch.device for computation\n",
        "        k: Number of cross-validation splits\n",
        "        n_val_users: Number of users for validation set\n",
        "        batch_size: Batch size for training\n",
        "        hidden_layers: Number of recurrent layers\n",
        "        hidden_size: Hidden state dimensionality\n",
        "        learning_rate: Learning rate for optimizer\n",
        "        dropout_rate: Dropout rate\n",
        "        window_size: Length of sliding windows\n",
        "        stride: Step size for sliding windows\n",
        "        rnn_type: Type of RNN ('RNN', 'LSTM', 'GRU')\n",
        "        bidirectional: Whether to use bidirectional RNN\n",
        "        l1_lambda: L1 regularization coefficient (if used)\n",
        "        l2_lambda: L2 regularization coefficient (weight_decay)\n",
        "        patience: Early stopping patience\n",
        "        evaluation_metric: Metric to monitor for early stopping\n",
        "        mode: 'max' or 'min' for evaluation metric\n",
        "        restore_best_weights: Whether to restore best weights after training\n",
        "        writer: TensorBoard writer\n",
        "        verbose: Verboisty level\n",
        "        seed: Random seed\n",
        "        experiment_name: Name for experiment logging\n",
        "        gradient_clipping_max_norm (float, optional): Max norm for gradient clipping. If None, no clipping.\n",
        "        pain_survey_embedding_dims (list, optional): List of embedding dimensions for pain survey features.\n",
        "\n",
        "    Returns:\n",
        "        fold_losses: Dict with validation losses for each split\n",
        "        fold_metrics: Dict with validation F1 scores for each split\n",
        "        best_scores: Dict with best F1 score for each split plus mean and std\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialise containers for results across all splits\n",
        "    fold_losses = {}\n",
        "    fold_metrics = {}\n",
        "    best_scores = {}\n",
        "\n",
        "    label_col = 'label'\n",
        "    user_id_col = 'sample_index'\n",
        "\n",
        "    # Get number of classes from the global df 'label' column\n",
        "    label_mapping = {\n",
        "        'no_pain': 0,\n",
        "        'low_pain': 1,\n",
        "        'high_pain': 2,\n",
        "    }\n",
        "    num_classes = len(label_mapping)\n",
        "\n",
        "    # Store initial weights to reset model for each split\n",
        "    # Model is created inside the loop to ensure fresh state for each split\n",
        "    initial_state = None\n",
        "\n",
        "    # Define all feature columns (dynamic + static) for sequence building and scaling\n",
        "    all_feature_cols = [col for col in df.columns if col.startswith('joint_')] + STATIC_COLS\n",
        "\n",
        "    # Define pain survey columns\n",
        "    pain_survey_cols = PAIN_SURVEY_FEATURES # Use global PAIN_SURVEY_FEATURES\n",
        "\n",
        "    # Iterate through K random splits\n",
        "    for split_idx in range(k):\n",
        "\n",
        "        if verbose > 0:\n",
        "            print(f\"Split {split_idx+1}/{k}\")\n",
        "\n",
        "        # Get unique user IDs and shuffle them with split-specific seed\n",
        "        unique_users = df[user_id_col].unique()\n",
        "        random.seed(seed + split_idx) # Use seed for reproducibility across splits\n",
        "        random.shuffle(unique_users)\n",
        "\n",
        "        # Calculate the number of users for the training set\n",
        "        n_train_users = len(unique_users) - n_val_users\n",
        "\n",
        "        # Split the shuffled user IDs into training, validation, and test sets\n",
        "        train_users = unique_users[:n_train_users]\n",
        "        val_users = unique_users[n_train_users:n_train_users + n_val_users]\n",
        "\n",
        "        # Create deep copies for each split to avoid modifying the original df or other splits\n",
        "        df_train_split = df[df[user_id_col].isin(train_users)].copy()\n",
        "        df_val_split = df[df[user_id_col].isin(val_users)].copy()\n",
        "\n",
        "        # Apply label mapping to convert string labels to integers for the current split\n",
        "        df_train_split['label'] = df_train_split['label'].map(label_mapping)\n",
        "        df_val_split['label'] = df_val_split['label'].map(label_mapping)\n",
        "\n",
        "        if verbose > 0:\n",
        "            print(f\"  Training set shape: {df_train_split.shape}\")\n",
        "            print(f\"  Validation set shape: {df_val_split.shape}\")\n",
        "\n",
        "        # Normalise all features (dynamic + static) using training set statistics for *this split*\n",
        "        # Calculate min/max only from the training split of the current fold\n",
        "        mins = df_train_split[all_feature_cols].min()\n",
        "        maxs = df_train_split[all_feature_cols].max()\n",
        "\n",
        "        # Apply normalization and ensure float32 for all numerical feature columns in all splits\n",
        "        for col in all_feature_cols:\n",
        "            # Handle potential division by zero if max equals min (constant column)\n",
        "            divisor = (maxs[col] - mins[col])\n",
        "            if divisor == 0: # If column is constant, assign 0 (or keep original value, depending on desired behavior)\n",
        "                df_train_split[col] = 0.0\n",
        "                df_val_split[col] = 0.0\n",
        "            else:\n",
        "                df_train_split[col] = ((df_train_split[col] - mins[col]) / divisor).astype(np.float32)\n",
        "                df_val_split[col] = ((df_val_split[col] - mins[col]) / divisor).astype(np.float32)\n",
        "\n",
        "        # Build sequences (main features and pain survey features)\n",
        "        X_train_sequences, X_train_ps_sequences, y_train_split, seq_ids_train_split = build_sequences(df_train_split, window=window_size, stride=stride,\n",
        "                                                                                                          feature_columns_to_extract=all_feature_cols,\n",
        "                                                                                                          pain_survey_columns_to_extract=pain_survey_cols)\n",
        "        X_val_sequences, X_val_ps_sequences, y_val_split, seq_ids_val_split     = build_sequences(df_val_split, window=window_size, stride=stride,\n",
        "                                                                                                      feature_columns_to_extract=all_feature_cols,\n",
        "                                                                                                      pain_survey_columns_to_extract=pain_survey_cols)\n",
        "\n",
        "        if verbose > 0:\n",
        "            print(f\"  Training main sequences shape: {X_train_sequences.shape}\")\n",
        "            print(f\"  Training PS sequences shape: {X_train_ps_sequences.shape}\")\n",
        "            print(f\"  Validation main sequences shape: {X_val_sequences.shape}\")\n",
        "            print(f\"  Validation PS sequences shape: {X_val_ps_sequences.shape}\")\n",
        "\n",
        "        # Create PyTorch datasets with combined features and pain survey features\n",
        "        train_ds = TensorDataset(torch.from_numpy(X_train_sequences), torch.from_numpy(X_train_ps_sequences).long(), torch.from_numpy(y_train_split).long())\n",
        "        val_ds   = TensorDataset(torch.from_numpy(X_val_sequences), torch.from_numpy(X_val_ps_sequences).long(), torch.from_numpy(y_val_split).long())\n",
        "\n",
        "        # Create data loaders\n",
        "        train_loader = make_loader(train_ds, batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "        val_loader   = make_loader(val_ds, batch_size=batch_size, shuffle=False, drop_last=False)\n",
        "\n",
        "        # The input size for the RNN is the number of features in the combined main sequences\n",
        "        rnn_input_size = X_train_sequences.shape[-1]\n",
        "\n",
        "        # Initialise model inside the loop to ensure fresh state for each split\n",
        "        model = RecurrentClassifier(\n",
        "            input_size=rnn_input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=hidden_layers,\n",
        "            num_classes=num_classes,\n",
        "            dropout_rate=dropout_rate,\n",
        "            bidirectional=bidirectional,\n",
        "            rnn_type=rnn_type,\n",
        "            pain_survey_embedding_dims=pain_survey_embedding_dims,\n",
        "            conv1d_out_channels=CONV1D_OUT_CHANNELS,\n",
        "            conv1d_kernel_size=CONV1D_KERNEL_SIZE,\n",
        "            conv1d_padding=CONV1D_PADDING\n",
        "        ).to(device)\n",
        "\n",
        "        # Calculate class weights for this split's training data\n",
        "        split_class_labels = np.unique(y_train_split)\n",
        "        split_class_weights_np = compute_class_weight(\n",
        "            class_weight='balanced',\n",
        "            classes=split_class_labels,\n",
        "            y=y_train_split\n",
        "        )\n",
        "        split_class_weights_tensor = torch.tensor(split_class_weights_np, dtype=torch.float32).to(device)\n",
        "\n",
        "        # Define criterion for this split with calculated weights\n",
        "        split_criterion = nn.CrossEntropyLoss(weight=split_class_weights_tensor, label_smoothing=LABEL_SMOOTHING)\n",
        "\n",
        "        # Define optimizer with L2 regularization\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=l2_lambda)\n",
        "\n",
        "        # Enable mixed precision training for GPU acceleration\n",
        "        split_scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
        "\n",
        "        # Create directory for model checkpoints\n",
        "        os.makedirs(f\"models/{experiment_name}\", exist_ok=True)\n",
        "\n",
        "        # Train model on current split\n",
        "        model, training_history = fit(\n",
        "            model=model,\n",
        "            train_loader=train_loader,\n",
        "            val_loader=val_loader,\n",
        "            epochs=epochs,\n",
        "            criterion=split_criterion, # Pass the split-specific criterion\n",
        "            optimizer=optimizer,\n",
        "            scaler=split_scaler,\n",
        "            device=device,\n",
        "            writer=writer,\n",
        "            patience=patience,\n",
        "            verbose=verbose,\n",
        "            l1_lambda=l1_lambda,\n",
        "            evaluation_metric=evaluation_metric,\n",
        "            mode=mode,\n",
        "            restore_best_weights=restore_best_weights,\n",
        "            experiment_name=experiment_name+\"/split_\"+str(split_idx),\n",
        "            gradient_clipping_max_norm=gradient_clipping_max_norm # Pass gradient clipping parameter\n",
        "        )\n",
        "\n",
        "        # Store results for this split\n",
        "        # Store the entire history for plotting purposes\n",
        "        fold_losses[f\"split_{split_idx}\"] = training_history['val_loss']\n",
        "        fold_metrics[f\"split_{split_idx}\"] = training_history['val_f1']\n",
        "\n",
        "        # Also store the best single F1 score for mean/std calculation\n",
        "        best_val_f1_in_split = max(training_history['val_f1'])\n",
        "        best_scores[f\"split_{split_idx}\"] = best_val_f1_in_split\n",
        "\n",
        "    # Compute mean and standard deviation of best scores across splits\n",
        "    best_scores[\"mean\"] = np.mean([best_scores[k] for k in best_scores.keys() if k.startswith(\"split_\")])\n",
        "    best_scores[\"std\"] = np.std([best_scores[k] for k in best_scores.keys() if k.startswith(\"split_\")])\n",
        "\n",
        "    if verbose > 0:\n",
        "        print(f\"Best score: {best_scores['mean']:.4f}\\u00b1{best_scores['std']:.4f}\")\n",
        "\n",
        "    return fold_losses, fold_metrics, best_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cw59lTVZHXTs"
      },
      "outputs": [],
      "source": [
        "# @title Training with k-fold\n",
        "%%time\n",
        "# Execute K-fold cross-validation with baseline configuration\n",
        "losses, metrics, best_scores = k_shuffle_split_cross_validation_round_rnn(\n",
        "    df=df,\n",
        "    epochs=EPOCHS,\n",
        "    criterion=criterion,\n",
        "    device=device,\n",
        "    k=K,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    hidden_layers=HIDDEN_LAYERS,\n",
        "    hidden_size=HIDDEN_SIZE,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    dropout_rate=DROPOUT_RATE,\n",
        "    l1_lambda=L1_LAMBDA,\n",
        "    l2_lambda=L2_LAMBDA,\n",
        "    verbose=VERBOSE,\n",
        "    patience=PATIENCE,\n",
        "    seed=SEED,\n",
        "    experiment_name=\"gru_baseline\",\n",
        "    n_val_users=N_VAL_USERS,\n",
        "    window_size=WINDOW_SIZE,\n",
        "    stride=STRIDE,\n",
        "    rnn_type=RNN_TYPE,\n",
        "    bidirectional=BIDIRECTIONAL\n",
        "    gradient_clipping_max_norm=GRADIENT_CLIPPING_MAX_NORM\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UQK2Wls-7BZ"
      },
      "source": [
        "# Assuming 'best_scores' and 'experiment_name' (from k-fold) are available\n",
        "# 'best_scores' is a dictionary containing F1 scores for each split and their mean/std.\n",
        "# 'experiment_name' for k-fold was 'gru_baseline'.\n",
        "\n",
        "# Find the split with the highest F1 score\n",
        "overall_best_f1_kfold = -1.0\n",
        "best_split_index_kfold = -1\n",
        "\n",
        "for i in range(K): # K is the number of splits from the cross-validation setup\n",
        "    current_split_f1 = best_scores[f'split_{i}']\n",
        "    if current_split_f1 > overall_best_f1_kfold:\n",
        "        overall_best_f1_kfold = current_split_f1\n",
        "        best_split_index_kfold = i\n",
        "\n",
        "# Define the source path for the best model from the best split\n",
        "source_model_filename = f\"gru_baseline/split_{best_split_index_kfold}_model.pt\"\n",
        "source_model_path = os.path.join(\"models\", source_model_filename)\n",
        "\n",
        "# Define the destination path for the overall best model\n",
        "destination_model_filename = f\"best_kfold_model_f1_{overall_best_f1_kfold:.4f}.pt\"\n",
        "destination_model_path = os.path.join(\"models\", destination_model_filename)\n",
        "\n",
        "# Ensure the source model file exists before copying\n",
        "if os.path.exists(source_model_path):\n",
        "    # Copy the best model from the best split to a new, descriptive filename\n",
        "    shutil.copyfile(source_model_path, destination_model_path)\n",
        "    print(f\"Overall best K-fold model (F1: {overall_best_f1_kfold:.4f} from split {best_split_index_kfold}) saved to: {destination_model_path}\")\n",
        "else:\n",
        "    print(f\"Error: Best model for split {best_split_index_kfold} not found at {source_model_path}\")\n",
        "    print(\"Please ensure the K-fold cross-validation training was executed successfully.\")\n",
        "\n",
        "# Update the best_performance variable for consistency if desired\n",
        "best_performance = overall_best_f1_kfold\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCzZsr7t-7Ba"
      },
      "source": [
        "# Create an inverse mapping from numeric labels back to string labels\n",
        "inverse_label_mapping = {\n",
        "    0: 'no_pain',\n",
        "    1: 'low_pain',\n",
        "    2: 'high_pain'\n",
        "}\n",
        "\n",
        "# The `test_ds` should already be defined correctly in n4g715TtSytf:\n",
        "# test_ds = TensorDataset(torch.from_numpy(X_test).float(), torch.from_numpy(X_test_static).float())\n",
        "# We don't redefine test_ds here.\n",
        "\n",
        "# Create DataLoader for the test set (using the already defined test_ds)\n",
        "test_loader = make_loader(test_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
        "\n",
        "#  Load the best K-fold model\n",
        "best_kfold_model_path = destination_model_path\n",
        "\n",
        "# Instantiate a new model with the architecture used for the best K-fold run\n",
        "# Parameters should match the best performing config from k-fold, which were set globally for the k-fold run\n",
        "# (HIDDEN_SIZE, HIDDEN_LAYERS, RNN_TYPE='GRU', BIDIRECTIONAL=True)\n",
        "model_for_prediction = RecurrentClassifier(\n",
        "    input_size=X_test.shape[-1],\n",
        "    hidden_size=HIDDEN_SIZE,\n",
        "    num_layers=HIDDEN_LAYERS,\n",
        "    num_classes=num_classes,\n",
        "    dropout_rate=DROPOUT_RATE,\n",
        "    bidirectional=BIDIRECTIONAL,\n",
        "    rnn_type=RNN_TYPE,\n",
        "    pain_survey_embedding_dims=PAIN_SURVEY_EMBEDDING_DIMS,\n",
        "    conv1d_out_channels=CONV1D_OUT_CHANNELS,\n",
        "    conv1d_kernel_size=CONV1D_KERNEL_SIZE,\n",
        "    conv1d_padding=CONV1D_PADDING\n",
        ").to(device)\n",
        "\n",
        "# Load the state dictionary of the best K-fold model\n",
        "model_for_prediction.load_state_dict(torch.load(best_kfold_model_path, map_location=device))\n",
        "model_for_prediction.eval() # Set the loaded model to evaluation mode\n",
        "\n",
        "all_test_predictions = []\n",
        "# The seq_ids_test array generated by build_sequences already maps sequence predictions to sample_index.\n",
        "# We will use this directly for grouping after predictions.\n",
        "\n",
        "# Perform inference on the test set using the loaded best K-fold model\n",
        "with torch.no_grad():\n",
        "    for inputs_batch, pain_survey_inputs_batch in test_loader: # Unpack only inputs\n",
        "        inputs_batch = inputs_batch.to(device)\n",
        "        pain_survey_inputs_batch = pain_survey_inputs_batch.to(device)\n",
        "\n",
        "        # Get model outputs\n",
        "        outputs = model_for_prediction(inputs_batch, pain_survey_inputs_batch)\n",
        "\n",
        "        # Get predicted class (the one with the highest probability)\n",
        "        predictions = outputs.argmax(dim=1).cpu().numpy()\n",
        "\n",
        "        all_test_predictions.extend(predictions)\n",
        "\n",
        "# Combine seq_ids_test and predictions into a DataFrame\n",
        "predictions_df = pd.DataFrame({\n",
        "    'sample_index': seq_ids_test, # Use the global seq_ids_test array\n",
        "    'predicted_label_numeric': all_test_predictions\n",
        "})\n",
        "\n",
        "# Group by sample_index and determine the final prediction\n",
        "# Add a check for empty group to avoid IndexError.\n",
        "final_predictions_grouped = predictions_df.groupby('sample_index')['predicted_label_numeric'].apply(\n",
        "    lambda x: x.mode()[0] if not x.empty else np.nan # If a group is empty, assign NaN\n",
        ")\n",
        "final_predictions_df = final_predictions_grouped.reset_index()\n",
        "final_predictions_df.columns = ['sample_index', 'predicted_label_numeric']\n",
        "\n",
        "# Map numeric predictions back to string labels\n",
        "final_predictions_df['predicted_label'] = final_predictions_df['predicted_label_numeric'].map(inverse_label_mapping)\n",
        "\n",
        "# Define the output filename\n",
        "output_filename = f'pirate_pain_predictions_{experiment_name}_cv.csv'\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "final_predictions_df[['sample_index', 'predicted_label']].to_csv(output_filename, index=False)\n",
        "\n",
        "print(f\"Predictions saved to {output_filename}\")\n",
        "display(final_predictions_df.head())\n",
        "\n",
        "# The file is already saved to Google Drive because the current working directory\n",
        "# is set to '/gdrive/My Drive/Challenge1-Datasets'. No need to copy it again.\n",
        "print(f\"File '{output_filename}' is already saved to Google Drive at '{current_dir}/{output_filename}'\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fq7WbKm0NuDv"
      },
      "outputs": [],
      "source": [
        "# @title Plot History\n",
        "# Create figure with two subplots sharing x axis\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(18, 5), sharex=True)\n",
        "\n",
        "# Color palette for K splits\n",
        "colors = plt.cm.get_cmap('tab10', K)\n",
        "\n",
        "# Plot validation loss for each split\n",
        "for split in range(K):\n",
        "    axes[0].plot(losses[f'split_{split}'][:-PATIENCE], label=f'Split {split+1}',\n",
        "                 color=colors(split), alpha=0.6)\n",
        "axes[0].set_title('Validation Loss per Split')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "# Plot validation F1 score for each split\n",
        "for split in range(K):\n",
        "    axes[1].plot(metrics[f'split_{split}'][:-PATIENCE], label=f'Split {split+1}',\n",
        "                 color=colors(split), alpha=0.6)\n",
        "axes[1].set_title('Validation F1 Score per Split')\n",
        "axes[1].set_ylabel('F1 Score')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "# Add shared legend on the right\n",
        "handles, labels = axes[0].get_legend_handles_labels()\n",
        "fig.legend(handles, labels, loc='center left', bbox_to_anchor=(1, 0.5), fontsize='small')\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(right=0.975)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Plot Confusion Matrix for Best K-fold Validation Set\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "\n",
        "# Re-create the validation data for the best performing split\n",
        "label_col = 'label'\n",
        "user_id_col = 'sample_index'\n",
        "label_mapping = {\n",
        "    'no_pain': 0,\n",
        "    'low_pain': 1,\n",
        "    'high_pain': 2,\n",
        "}\n",
        "# num_classes should be available globally, but we can re-derive it safely\n",
        "num_classes_derived = len(label_mapping)\n",
        "\n",
        "# Re-run the user splitting logic for the best split\n",
        "unique_users = df[user_id_col].unique()\n",
        "random.seed(SEED + best_split_index_kfold) # Use the same seed as that split\n",
        "random.shuffle(unique_users)\n",
        "\n",
        "n_train_users_best_split = len(unique_users) - N_VAL_USERS\n",
        "train_users_best_split = unique_users[:n_train_users_best_split]\n",
        "val_users_best_split = unique_users[n_train_users_best_split:n_train_users_best_split + N_VAL_USERS]\n",
        "\n",
        "df_train_split_best = df[df[user_id_col].isin(train_users_best_split)].copy()\n",
        "df_val_split_best = df[df[user_id_col].isin(val_users_best_split)].copy()\n",
        "\n",
        "# Apply label mapping\n",
        "df_train_split_best['label'] = df_train_split_best['label'].map(label_mapping)\n",
        "df_val_split_best['label'] = df_val_split_best['label'].map(label_mapping)\n",
        "\n",
        "# Dynamically identify all feature columns (dynamic + static) for scaling and sequence building for this split\n",
        "all_feature_cols_best_split = [col for col in df_train_split_best.columns if col.startswith('joint_')] + STATIC_COLS\n",
        "\n",
        "# Pain survey columns\n",
        "pain_survey_cols_cm = PAIN_SURVEY_FEATURES\n",
        "\n",
        "# Normalize features using training set statistics for this specific split\n",
        "mins_best_split = df_train_split_best[all_feature_cols_best_split].min()\n",
        "maxs_best_split = df_train_split_best[all_feature_cols_best_split].max()\n",
        "\n",
        "for col in all_feature_cols_best_split:\n",
        "    divisor = (maxs_best_split[col] - mins_best_split[col])\n",
        "    if divisor == 0: # If column is constant, assign 0.0 for consistent typing\n",
        "        df_train_split_best[col] = 0.0\n",
        "        df_val_split_best[col] = 0.0\n",
        "    else:\n",
        "        df_train_split_best[col] = ((df_train_split_best[col] - mins_best_split[col]) / divisor).astype(np.float32)\n",
        "        df_val_split_best[col] = ((df_val_split_best[col] - mins_best_split[col]) / divisor).astype(np.float32)\n",
        "\n",
        "# Build sequences (main features and pain survey features)\n",
        "X_val_sequences_best_split, X_val_ps_sequences_best_split, y_val_best_split, seq_ids_val_best_split = build_sequences(\n",
        "    df_val_split_best, WINDOW_SIZE, STRIDE,\n",
        "    feature_columns_to_extract=all_feature_cols_best_split,\n",
        "    pain_survey_columns_to_extract=pain_survey_cols_cm\n",
        ")\n",
        "\n",
        "# Create DataLoader for the validation set of the best split\n",
        "val_ds_best_split = TensorDataset(torch.from_numpy(X_val_sequences_best_split), torch.from_numpy(X_val_ps_sequences_best_split).long(), torch.from_numpy(y_val_best_split).long())\n",
        "val_loader_best_split = make_loader(val_ds_best_split, BATCH_SIZE, shuffle=False, drop_last=False)\n",
        "\n",
        "# Load the best K-fold model (destination_model_path should be available from earlier execution)\n",
        "model_for_conf_matrix = RecurrentClassifier(\n",
        "    input_size=X_val_sequences_best_split.shape[-1], # Use the dynamically determined number of combined features\n",
        "    hidden_size=HIDDEN_SIZE,\n",
        "    num_layers=HIDDEN_LAYERS,\n",
        "    num_classes=num_classes_derived,\n",
        "    dropout_rate=DROPOUT_RATE,\n",
        "    bidirectional=BIDIRECTIONAL,\n",
        "    rnn_type=RNN_TYPE,\n",
        "    pain_survey_embedding_dims=PAIN_SURVEY_EMBEDDING_DIMS, # Pass embedding dims\n",
        "    conv1d_out_channels=CONV1D_OUT_CHANNELS,\n",
        "    conv1d_kernel_size=CONV1D_KERNEL_SIZE,\n",
        "    conv1d_padding=CONV1D_PADDING\n",
        ").to(device)\n",
        "\n",
        "model_for_conf_matrix.load_state_dict(torch.load(destination_model_path, map_location=device))\n",
        "model_for_conf_matrix.eval()\n",
        "\n",
        "# Collect predictions and ground truth labels for the best split's validation set\n",
        "val_preds_best_split, val_targets_best_split = [], []\n",
        "with torch.no_grad():\n",
        "    for xb, xb_ps, yb in val_loader_best_split:\n",
        "        xb = xb.to(device)\n",
        "        xb_ps = xb_ps.to(device)\n",
        "        logits = model_for_conf_matrix(xb, xb_ps)\n",
        "        preds = logits.argmax(dim=1).cpu().numpy()\n",
        "        val_preds_best_split.append(preds)\n",
        "        val_targets_best_split.append(yb.numpy())\n",
        "\n",
        "val_preds_best_split = np.concatenate(val_preds_best_split)\n",
        "val_targets_best_split = np.concatenate(val_targets_best_split)\n",
        "\n",
        "# Calculate overall validation metrics for this split\n",
        "val_acc = accuracy_score(val_targets_best_split, val_preds_best_split)\n",
        "val_prec = precision_score(val_targets_best_split, val_preds_best_split, average='weighted', zero_division=0)\n",
        "val_rec = recall_score(val_targets_best_split, val_preds_best_split, average='weighted', zero_division=0)\n",
        "val_f1 = f1_score(val_targets_best_split, val_preds_best_split, average='weighted', zero_division=0)\n",
        "print(f\"Metrics for Validation Set of Best K-fold Split (Fold {best_split_index_kfold+1}):\")\n",
        "print(f\"Accuracy: {val_acc:.4f}\")\n",
        "print(f\"Precision: {val_prec:.4f}\")\n",
        "print(f\"Recall: {val_rec:.4f}\")\n",
        "print(f\"F1 score: {val_f1:.4f}\")\n",
        "\n",
        "# Generate confusion matrix\n",
        "cm_best_split = confusion_matrix(val_targets_best_split, val_preds_best_split)\n",
        "\n",
        "# Map numeric labels back to string labels for better readability in the plot\n",
        "inverse_label_mapping = {0: 'no_pain', 1: 'low_pain', 2: 'high_pain'}\n",
        "class_labels = [inverse_label_mapping[i] for i in sorted(inverse_label_mapping.keys())]\n",
        "\n",
        "# Visualise confusion matrix\n",
        "plt.figure(figsize=(8, 7))\n",
        "sns.heatmap(cm_best_split, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=class_labels, yticklabels=class_labels\n",
        "           )\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title(f'Confusion Matrix â€” Validation Set (Best K-fold Split Fold {best_split_index_kfold+1})')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ro6_LsnxFrwN",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HYPERPARAMETER TUNING"
      ],
      "metadata": {
        "id": "xUYKXEJ12zCB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPlepEPwHXOo"
      },
      "outputs": [],
      "source": [
        "def grid_search_cv_rnn(df, param_grid, fixed_params, cv_params, verbose=True):\n",
        "    \"\"\"\n",
        "    Execute grid search with K-shuffle-split cross-validation for RNN models on time series data.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame with columns ['user_id', 'activity', 'x_axis', 'y_axis', 'z_axis', 'id']\n",
        "        param_grid: Dict of parameters to test, e.g. {'batch_size': [16, 32], 'rnn_type': ['LSTM', 'GRU']}\n",
        "        fixed_params: Dict of fixed hyperparameters (hidden_size, learning_rate, window_size, stride, etc.)\n",
        "        cv_params: Dict of CV settings (epochs, k, patience, criterion, scaler, device, etc.)\n",
        "        verbose: Print progress for each configuration\n",
        "\n",
        "    Returns:\n",
        "        results: Dict with scores for each configuration\n",
        "        best_config: Dict with best hyperparameter combination\n",
        "        best_score: Best mean F1 score achieved\n",
        "    \"\"\"\n",
        "    # Generate all parameter combinations\n",
        "    param_names = list(param_grid.keys())\n",
        "    param_values = list(param_grid.values())\n",
        "    combinations = list(product(*param_values))\n",
        "\n",
        "    results = {}\n",
        "    best_score = -np.inf\n",
        "    best_config = None\n",
        "\n",
        "    total = len(combinations)\n",
        "\n",
        "    for idx, combo in enumerate(combinations, 1):\n",
        "        # Create current configuration dict\n",
        "        current_config = dict(zip(param_names, combo))\n",
        "        config_str = \"_\".join([f\"{k}_{v}\" for k, v in current_config.items()])\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\nConfiguration {idx}/{total}:\")\n",
        "            for param, value in current_config.items():\n",
        "                print(f\"  {param}: {value}\")\n",
        "\n",
        "        # Merge current config with fixed parameters\n",
        "        run_params = {**fixed_params, **current_config}\n",
        "\n",
        "        # Execute cross-validation\n",
        "        _, _, fold_scores = k_shuffle_split_cross_validation_round_rnn(\n",
        "            df=df,\n",
        "            experiment_name=config_str,\n",
        "            **run_params,\n",
        "            **cv_params\n",
        "        )\n",
        "\n",
        "        # Store results\n",
        "        results[config_str] = fold_scores\n",
        "\n",
        "        # Track best configuration\n",
        "        if fold_scores[\"mean\"] > best_score:\n",
        "            best_score = fold_scores[\"mean\"]\n",
        "            best_config = current_config.copy()\n",
        "            if verbose:\n",
        "                print(\"  NEW BEST SCORE!\")\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"  F1 Score: {fold_scores['mean']:.4f}Â±{fold_scores['std']:.4f}\")\n",
        "\n",
        "    return results, best_config, best_score\n",
        "\n",
        "\n",
        "def plot_top_configurations_rnn(results, k_splits, top_n=5, figsize=(14, 7)):\n",
        "    \"\"\"\n",
        "    Visualise top N RNN configurations with boxplots of F1 scores across CV splits.\n",
        "\n",
        "    Args:\n",
        "        results: Dict of results from grid_search_cv_rnn\n",
        "        k_splits: Number of CV splits used\n",
        "        top_n: Number of top configurations to display\n",
        "        figsize: Figure size tuple\n",
        "    \"\"\"\n",
        "    # Sort by mean score\n",
        "    config_scores = {name: data['mean'] for name, data in results.items()}\n",
        "    sorted_configs = sorted(config_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Select top N\n",
        "    top_configs = sorted_configs[:min(top_n, len(sorted_configs))]\n",
        "\n",
        "    # Prepare boxplot data\n",
        "    boxplot_data = []\n",
        "    labels = []\n",
        "\n",
        "    # Define a dictionary for replacements, ordered to handle prefixes correctly\n",
        "    replacements = {\n",
        "        'batch_size_': 'BS=',\n",
        "        'learning_rate_': '\\nLR=',\n",
        "        'hidden_layers_': '\\nHL=',\n",
        "        'hidden_size_': '\\nHS=',\n",
        "        'dropout_rate_': '\\nDR=',\n",
        "        'window_size_': '\\nWS=',\n",
        "        'stride_': '\\nSTR=',\n",
        "        'rnn_type_': '\\nRNN=',\n",
        "        'bidirectional_': '\\nBIDIR=',\n",
        "        'l1_lambda_': '\\nL1=',\n",
        "        'l2_lambda_': '\\nL2='\n",
        "    }\n",
        "\n",
        "    # Replacements for separators\n",
        "    separator_replacements = {\n",
        "        '_learning_rate_': '\\nLR=',\n",
        "        '_hidden_layers_': '\\nHL=',\n",
        "        '_hidden_size_': '\\nHS=',\n",
        "        '_dropout_rate_': '\\nDR=',\n",
        "        '_window_size_': '\\nWS=',\n",
        "        '_stride_': '\\nSTR=',\n",
        "        '_rnn_type_': '\\nRNN=',\n",
        "        '_bidirectional_': '\\nBIDIR=',\n",
        "        '_l1_lambda_': '\\nL1=',\n",
        "        '_l2_lambda_': '\\nL2=',\n",
        "        '_': ''\n",
        "    }\n",
        "\n",
        "    for config_name, mean_score in top_configs:\n",
        "        # Extract best score from each split (auto-detect number of splits)\n",
        "        split_scores = []\n",
        "        for i in range(k_splits):\n",
        "            if f'split_{i}' in results[config_name]:\n",
        "                split_scores.append(results[config_name][f'split_{i}'])\n",
        "        boxplot_data.append(split_scores)\n",
        "\n",
        "        # Verify we have the expected number of splits\n",
        "        if len(split_scores) != k_splits:\n",
        "            print(f\"Warning: Config {config_name} has {len(split_scores)} splits, expected {k_splits}\")\n",
        "\n",
        "        # Create readable label using the replacements dictionary\n",
        "        readable_label = config_name\n",
        "        for old, new in replacements.items():\n",
        "            readable_label = readable_label.replace(old, new)\n",
        "\n",
        "        # Apply separator replacements\n",
        "        for old, new in separator_replacements.items():\n",
        "             readable_label = readable_label.replace(old, new)\n",
        "\n",
        "        labels.append(f\"{readable_label}\\n(Î¼={mean_score:.3f})\")\n",
        "\n",
        "    # Create plot\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    bp = ax.boxplot(boxplot_data, labels=labels, patch_artist=True,\n",
        "                    showmeans=True, meanline=True)\n",
        "\n",
        "    # Styling\n",
        "    for patch in bp['boxes']:\n",
        "        patch.set_facecolor('lightblue')\n",
        "        patch.set_alpha(0.7)\n",
        "\n",
        "    # Highlight best configuration\n",
        "    ax.get_xticklabels()[0].set_fontweight('bold')\n",
        "\n",
        "    ax.set_ylabel('F1 Score')\n",
        "    ax.set_xlabel('Configuration')\n",
        "    ax.set_title(f'Top {len(top_configs)} RNN Configurations - F1 Score Distribution Across {k_splits} Splits')\n",
        "    ax.grid(alpha=0.3, axis='y')\n",
        "\n",
        "    plt.xticks(rotation=0, ha='center')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbD8awRDHXML"
      },
      "outputs": [],
      "source": [
        "# @title Grid search\n",
        "%%time\n",
        "# Define parameters to search\n",
        "param_grid = {\n",
        "    'window_size': [60, 90],\n",
        "    'stride': [10, 15, 30],\n",
        "}\n",
        "\n",
        "# Fixed hyperparameters (not being tuned)\n",
        "fixed_params = {\n",
        "    'batch_size': BATCH_SIZE,\n",
        "    'learning_rate': LEARNING_RATE,\n",
        "    'hidden_layers': HIDDEN_LAYERS,\n",
        "    'hidden_size': HIDDEN_SIZE,\n",
        "    'dropout_rate': DROPOUT_RATE,\n",
        "    'l1_lambda': L1_LAMBDA,\n",
        "    'l2_lambda': L2_LAMBDA,\n",
        "    'rnn_type': RNN_TYPE,\n",
        "    'bidirectional': BIDIRECTIONAL,\n",
        "    'pain_survey_embedding_dims': pain_survey_embedding_dims\n",
        "}\n",
        "\n",
        "# Cross-validation settings\n",
        "cv_params = {\n",
        "    'epochs': EPOCHS,\n",
        "    'criterion': criterion,\n",
        "    'device': device,\n",
        "    'k': K,\n",
        "    'n_val_users': N_VAL_USERS,\n",
        "    'patience': PATIENCE,\n",
        "    'verbose': 0,\n",
        "    'seed': SEED,\n",
        "    'gradient_clipping_max_norm': GRADIENT_CLIPPING_MAX_NORM # Pass gradient clipping parameter\n",
        "}\n",
        "\n",
        "# Execute search\n",
        "results, best_config, best_score = grid_search_cv_rnn(\n",
        "    df=df,\n",
        "    param_grid=param_grid,\n",
        "    fixed_params=fixed_params,\n",
        "    cv_params=cv_params\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImYxq6AsHXJv"
      },
      "outputs": [],
      "source": [
        "# Visualise results\n",
        "plot_top_configurations_rnn(results, k_splits=K, top_n=5)"
      ]
    }
  ]
}